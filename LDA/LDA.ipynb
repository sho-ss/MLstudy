{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished making vocab\n",
      "start iterating\n",
      "1 iter finished !\n",
      "2 iter finished !\n",
      "3 iter finished !\n",
      "4 iter finished !\n",
      "5 iter finished !\n",
      "6 iter finished !\n",
      "7 iter finished !\n",
      "8 iter finished !\n",
      "9 iter finished !\n",
      "10 iter finished !\n",
      "11 iter finished !\n",
      "12 iter finished !\n",
      "13 iter finished !\n",
      "14 iter finished !\n",
      "15 iter finished !\n",
      "16 iter finished !\n",
      "17 iter finished !\n",
      "18 iter finished !\n",
      "19 iter finished !\n",
      "20 iter finished !\n",
      "21 iter finished !\n",
      "22 iter finished !\n",
      "23 iter finished !\n",
      "24 iter finished !\n",
      "25 iter finished !\n",
      "26 iter finished !\n",
      "27 iter finished !\n",
      "28 iter finished !\n",
      "29 iter finished !\n",
      "30 iter finished !\n",
      "31 iter finished !\n",
      "32 iter finished !\n",
      "33 iter finished !\n",
      "34 iter finished !\n",
      "35 iter finished !\n",
      "36 iter finished !\n",
      "37 iter finished !\n",
      "38 iter finished !\n",
      "39 iter finished !\n",
      "40 iter finished !\n",
      "41 iter finished !\n",
      "42 iter finished !\n",
      "43 iter finished !\n",
      "44 iter finished !\n",
      "45 iter finished !\n",
      "46 iter finished !\n",
      "47 iter finished !\n",
      "48 iter finished !\n",
      "49 iter finished !\n",
      "50 iter finished !\n",
      "51 iter finished !\n",
      "52 iter finished !\n",
      "53 iter finished !\n",
      "54 iter finished !\n",
      "55 iter finished !\n",
      "56 iter finished !\n",
      "57 iter finished !\n",
      "58 iter finished !\n",
      "59 iter finished !\n",
      "60 iter finished !\n",
      "61 iter finished !\n",
      "62 iter finished !\n",
      "63 iter finished !\n",
      "64 iter finished !\n",
      "65 iter finished !\n",
      "66 iter finished !\n",
      "67 iter finished !\n",
      "68 iter finished !\n",
      "69 iter finished !\n",
      "70 iter finished !\n",
      "71 iter finished !\n",
      "72 iter finished !\n",
      "73 iter finished !\n",
      "74 iter finished !\n",
      "75 iter finished !\n",
      "76 iter finished !\n",
      "77 iter finished !\n",
      "78 iter finished !\n",
      "79 iter finished !\n",
      "80 iter finished !\n",
      "81 iter finished !\n",
      "82 iter finished !\n",
      "83 iter finished !\n",
      "84 iter finished !\n",
      "85 iter finished !\n",
      "86 iter finished !\n",
      "87 iter finished !\n",
      "88 iter finished !\n",
      "89 iter finished !\n",
      "90 iter finished !\n",
      "91 iter finished !\n",
      "92 iter finished !\n",
      "93 iter finished !\n",
      "94 iter finished !\n",
      "95 iter finished !\n",
      "96 iter finished !\n",
      "97 iter finished !\n",
      "98 iter finished !\n",
      "99 iter finished !\n",
      "100 iter finished !\n",
      "finished making vocab\n",
      "start iterating\n",
      "1 iter finished !\n",
      "2 iter finished !\n",
      "3 iter finished !\n",
      "4 iter finished !\n",
      "5 iter finished !\n",
      "6 iter finished !\n",
      "7 iter finished !\n",
      "8 iter finished !\n",
      "9 iter finished !\n",
      "10 iter finished !\n",
      "11 iter finished !\n",
      "12 iter finished !\n",
      "13 iter finished !\n",
      "14 iter finished !\n",
      "15 iter finished !\n",
      "16 iter finished !\n",
      "17 iter finished !\n",
      "18 iter finished !\n",
      "19 iter finished !\n",
      "20 iter finished !\n",
      "21 iter finished !\n",
      "22 iter finished !\n",
      "23 iter finished !\n",
      "24 iter finished !\n",
      "25 iter finished !\n",
      "26 iter finished !\n",
      "27 iter finished !\n",
      "28 iter finished !\n",
      "29 iter finished !\n",
      "30 iter finished !\n",
      "31 iter finished !\n",
      "32 iter finished !\n",
      "33 iter finished !\n",
      "34 iter finished !\n",
      "35 iter finished !\n",
      "36 iter finished !\n",
      "37 iter finished !\n",
      "38 iter finished !\n",
      "39 iter finished !\n",
      "40 iter finished !\n",
      "41 iter finished !\n",
      "42 iter finished !\n",
      "43 iter finished !\n",
      "44 iter finished !\n",
      "45 iter finished !\n",
      "46 iter finished !\n",
      "47 iter finished !\n",
      "48 iter finished !\n",
      "49 iter finished !\n",
      "50 iter finished !\n",
      "51 iter finished !\n",
      "52 iter finished !\n",
      "53 iter finished !\n",
      "54 iter finished !\n",
      "55 iter finished !\n",
      "56 iter finished !\n",
      "57 iter finished !\n",
      "58 iter finished !\n",
      "59 iter finished !\n",
      "60 iter finished !\n",
      "61 iter finished !\n",
      "62 iter finished !\n",
      "63 iter finished !\n",
      "64 iter finished !\n",
      "65 iter finished !\n",
      "66 iter finished !\n",
      "67 iter finished !\n",
      "68 iter finished !\n",
      "69 iter finished !\n",
      "70 iter finished !\n",
      "71 iter finished !\n",
      "72 iter finished !\n",
      "73 iter finished !\n",
      "74 iter finished !\n",
      "75 iter finished !\n",
      "76 iter finished !\n",
      "77 iter finished !\n",
      "78 iter finished !\n",
      "79 iter finished !\n",
      "80 iter finished !\n",
      "81 iter finished !\n",
      "82 iter finished !\n",
      "83 iter finished !\n",
      "84 iter finished !\n",
      "85 iter finished !\n",
      "86 iter finished !\n",
      "87 iter finished !\n",
      "88 iter finished !\n",
      "89 iter finished !\n",
      "90 iter finished !\n",
      "91 iter finished !\n",
      "92 iter finished !\n",
      "93 iter finished !\n",
      "94 iter finished !\n",
      "95 iter finished !\n",
      "96 iter finished !\n",
      "97 iter finished !\n",
      "98 iter finished !\n",
      "99 iter finished !\n",
      "100 iter finished !\n",
      "-----topic 0-----\n",
      "アプリ, pdf:0.039622402548982985\n",
      "更新, pdf:0.038495096024737614\n",
      "ソフトウェア, pdf:0.030132397516327916\n",
      "表示, pdf:0.023402493431057998\n",
      "画面, pdf:0.021441965829423607\n",
      "場合, pdf:0.018588506759170783\n",
      "設定, pdf:0.01564888314797775\n",
      "ダウンロード, pdf:0.013234116966581819\n",
      "とき, pdf:0.011869509832517207\n",
      "アップデート, pdf:0.01106421376489478\n",
      "-----topic 1-----\n",
      "さん, pdf:0.04094089095625374\n",
      "私, pdf:0.021978145393498648\n",
      "女性, pdf:0.021566337999796386\n",
      "それ, pdf:0.020160932502998812\n",
      "結婚, pdf:0.01786306591761682\n",
      "いい, pdf:0.0172742148021902\n",
      "仕事, pdf:0.016989804074622968\n",
      "何, pdf:0.016023447962948738\n",
      "方, pdf:0.015758640388855244\n",
      "時, pdf:0.013828008917644624\n",
      "-----topic 2-----\n",
      "D, pdf:0.032728176628356996\n",
      "発売, pdf:0.03043909770410264\n",
      "充電, pdf:0.017592102046798744\n",
      "スマホ, pdf:0.01611274988610853\n",
      "チェック, pdf:0.015881910662986658\n",
      "登場, pdf:0.014434952771402847\n",
      "iPhone, pdf:0.013222252115178652\n",
      "シリーズ, pdf:0.011527349846774462\n",
      "ユーザー, pdf:0.010650685517309274\n",
      "機器, pdf:0.01038806619386888\n",
      "-----topic 3-----\n",
      "円, pdf:0.013530464928143917\n",
      "月日, pdf:0.013179970422532689\n",
      "女性, pdf:0.012585749742973293\n",
      "特集, pdf:0.009915760533598437\n",
      "写真, pdf:0.008483385524182317\n",
      "店, pdf:0.008338024251696987\n",
      "紹介, pdf:0.0075490414363836265\n",
      "時, pdf:0.00653584014497045\n",
      "プレゼント, pdf:0.0062633016254161344\n",
      "魅力, pdf:0.006036711980764885\n",
      "-----topic 4-----\n",
      "映画, pdf:0.03903519685634752\n",
      "作品, pdf:0.02180482165103852\n",
      "日本, pdf:0.01978099130444395\n",
      "公開, pdf:0.017826683237117502\n",
      "監督, pdf:0.017317339827282462\n",
      "たち, pdf:0.011707635121536742\n",
      "世界, pdf:0.011332216671572989\n",
      "登場, pdf:0.010499520367519869\n",
      "本作, pdf:0.010474373853327005\n",
      "撮影, pdf:0.009236265304893118\n",
      "-----topic 5-----\n",
      "対応, pdf:0.03449850518565703\n",
      "利用, pdf:0.02692949809692747\n",
      "Android, pdf:0.01956456040689507\n",
      "ゴルフ, pdf:0.019253393632634376\n",
      "可能, pdf:0.014942066474554\n",
      "時間, pdf:0.014844073408504512\n",
      "機能, pdf:0.014264490837617381\n",
      "バッテリー, pdf:0.014185076279978176\n",
      "製品, pdf:0.014125941534963703\n",
      "PC, pdf:0.012429636302698466\n",
      "-----topic 6-----\n",
      "転職, pdf:0.021625035240473642\n",
      "通, pdf:0.01897817089451299\n",
      "情報, pdf:0.016433221413623543\n",
      "livedoor, pdf:0.012856638069220489\n",
      "日本, pdf:0.012789241077016539\n",
      "サービス, pdf:0.012351319063186412\n",
      "デジタル, pdf:0.011588840770957561\n",
      "万, pdf:0.011339375047058101\n",
      "会社, pdf:0.009327018996206934\n",
      "DEJI, pdf:0.009179968309881242\n",
      "-----topic 7-----\n",
      "歳, pdf:0.03295158163276728\n",
      "さん, pdf:0.02995918443603806\n",
      "男性, pdf:0.02001271995841228\n",
      "今, pdf:0.015496466451699017\n",
      "彼, pdf:0.014306928802692069\n",
      "そう, pdf:0.013252754336099676\n",
      "男, pdf:0.01220758626591772\n",
      "女, pdf:0.01143750964893318\n",
      "好き, pdf:0.011217739301110327\n",
      "独女, pdf:0.010777360780154177\n",
      "-----topic 8-----\n",
      "smartphone, pdf:0.03681678182954138\n",
      "搭載, pdf:0.03332402472495318\n",
      "S, pdf:0.025399417858749666\n",
      "円, pdf:0.019140660212345825\n",
      "機能, pdf:0.01888885365093197\n",
      "発表, pdf:0.018006009741620106\n",
      "モデル, pdf:0.01468753290964989\n",
      "向け, pdf:0.014248928438023105\n",
      "機種, pdf:0.013792012658968105\n",
      "MAX, pdf:0.01321268628035708\n",
      "-----topic 9-----\n",
      "話題, pdf:0.031323393848395414\n",
      "氏, pdf:0.016775348869561658\n",
      "月日, pdf:0.014261560930945966\n",
      "声, pdf:0.01390660518772593\n",
      "位, pdf:0.01268543786306519\n",
      "Twitter, pdf:0.011406227040190585\n",
      "放送, pdf:0.009703669359688607\n",
      "回, pdf:0.00911500582586705\n",
      "コメント, pdf:0.008531139977602704\n",
      "ネット掲示板, pdf:0.008490051149054694\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "import scipy.special as special\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import dill\n",
    "\n",
    "class LDA(object):\n",
    "    PRINT_EVERYITER = 1\n",
    "    \n",
    "    def __init__(self, n_iter, n_topic, max_df=0.3, min_df=0.01, lang='ja'):\n",
    "        self.n_iter = n_iter\n",
    "        self.n_topic = n_topic\n",
    "        self.max_df = max_df\n",
    "        self.min_df = min_df\n",
    "        self.topics = [i for i in range(self.n_topic)]\n",
    "        self.alpha = None\n",
    "        self.beta = None\n",
    "        self.lang = lang\n",
    "        \n",
    "    def __sampling_theta(self, alpha):\n",
    "        return stats.dirichlet.rvs(alpha=alpha)[0]\n",
    "    \n",
    "    def __sampling_phi(self, beta):\n",
    "        return stats.dirichlet.rvs(alpha=beta)[0]\n",
    "    \n",
    "    def __sampling_z(self, d, v):\n",
    "        \"\"\"\n",
    "        @param d int 文章番号\n",
    "        @param v int 文章dのi番目の単語の単語辞書での番号\n",
    "        \n",
    "        @return k int 単語vのトピック\n",
    "        \"\"\"\n",
    "        theta_d = self.theta[d]\n",
    "        phi_v = self.phi[:, v]\n",
    "        sum_weight = np.dot(theta_d, phi_v.reshape(-1, 1))\n",
    "        weight = theta_d*phi_v / sum_weight\n",
    "        \n",
    "        return np.random.choice(self.topics, size=1, p=weight)\n",
    "    \n",
    "    def __init_dirichlet_param(self):\n",
    "        self.alpha = np.array([10 / self.n_topic\n",
    "                               for _ in range(self.n_topic)])\n",
    "        self.beta = np.array([100 / self.vocab_size\n",
    "                              for _ in range(self.vocab_size)])\n",
    "    \n",
    "    def __update_dirichlet_param(self, ave_n_d_k):\n",
    "        \"\"\"\n",
    "        update alpha\n",
    "        \n",
    "        @param ave_n_d_k ndarray n_d_kのサンプル平均\n",
    "        \"\"\"\n",
    "        alpha_sum = np.sum(self.alpha)\n",
    "        \n",
    "        # alphaのupdate\n",
    "        for k in range(self.n_topic):\n",
    "            alpha_k = self.alpha[k]\n",
    "            \n",
    "            a_k_d = [(special.psi(ave_n_d_k[d, k] + alpha_k)\\\n",
    "                      - special.psi(alpha_k)) * alpha_k\n",
    "                     for d in range(self.n_docs)]\n",
    "            \n",
    "            b_d = [special.psi(np.sum(ave_n_d_k[:,k]) + alpha_sum)\\\n",
    "                   - special.psi(alpha_sum)\n",
    "                   for _ in range(self.n_docs)]\n",
    "            \n",
    "            self.alpha[k] = np.sum(a_k_d) / np.sum(b_d)\n",
    "    \n",
    "    def __make_vocab(self, X):\n",
    "        \"\"\"\n",
    "        文書集合から単語辞書を作成し、文書をbag-of-wordsに変換\n",
    "        \n",
    "        @param X list 文書集合 要素は、各文章を単語に分割したもの\n",
    "        \n",
    "        @return vocab dict 文書内の単語辞書\n",
    "        @return X_bow list 文書をbag-of-wordsで表現した文書集合\n",
    "        \"\"\"\n",
    "        if self.lang == 'ja':\n",
    "            # 文書は既に単語の集合に変換されているので、splitする必要がない。\n",
    "            analyzer = lambda words: words\n",
    "            self.vect = CountVectorizer(min_df=self.min_df,\n",
    "                                        max_df=self.max_df,\n",
    "                                        analyzer=analyzer)\n",
    "        else:\n",
    "            self.vect = CountVectorizer(max_features=10000,\n",
    "                                        max_df=self.max_df)\n",
    "        X_bow = self.vect.fit_transform(X).toarray()\n",
    "        return self.vect.vocabulary_, X_bow\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        self.vocabを求める関数未実装\n",
    "        \n",
    "        @param X list 文書集合\n",
    "        @return self 学習済みモデル\n",
    "        \"\"\"\n",
    "        # 辞書作成と文書をbag-of-wordsに変換\n",
    "        self.vocab, X_bow = self.__make_vocab(X)\n",
    "        print('finished making vocab')\n",
    "        \n",
    "        self.n_docs = len(X_bow)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "        self.__init_dirichlet_param()\n",
    "        \n",
    "        # θとφの初期化\n",
    "        # theta ndarray theta[d,k] 文章dにトピックkが出現する確率を保持\n",
    "        self.theta = np.array([self.__sampling_theta(alpha=self.alpha)\n",
    "                               for _ in range(self.n_docs)])\n",
    "        \n",
    "        # phi ndarray phi[k,v] トピックkに単語vが含まれている確率を保持\n",
    "        self.phi = np.array([self.__sampling_phi(beta=self.beta)\n",
    "                             for _ in range(self.n_topic)])\n",
    "        \n",
    "        # n_d_k ndarray n_d_k[d,k] 文章d出現したトピックkの数\n",
    "        self.n_d_k = np.zeros((self.n_docs, self.n_topic))\n",
    "        \n",
    "        # n_k_v ndarray n_k_b[k,v] トピックがkの単語vの個数\n",
    "        self.n_k_v = np.zeros((self.n_topic, self.vocab_size))\n",
    "        \n",
    "        # n_d_kのこれまでのサンプル合計\n",
    "        sum_n_d_k = np.zeros((self.n_docs, self.n_topic))\n",
    "        \n",
    "        print('start iterating')\n",
    "        for s in range(self.n_iter):\n",
    "            # 初期化\n",
    "            self.n_d_k.fill(0.0)\n",
    "            self.n_k_v.fill(0.0)\n",
    "            \n",
    "            for d in range(self.n_docs):\n",
    "                for v, n_word in enumerate(X_bow[d]):\n",
    "                    if n_word ==0:\n",
    "                        continue\n",
    "                    \n",
    "                    # 単語d,i=vのトピック\n",
    "                    k = self.__sampling_z(d, v)\n",
    "                    \n",
    "                    # それぞれの個数をプラス1\n",
    "                    self.n_k_v[k, v] += n_word\n",
    "                    self.n_d_k[d, k] += n_word\n",
    "                \n",
    "                # 更新されたn_d_kから、θ_dをサンプリング\n",
    "                params = self.n_d_k[d] + self.alpha\n",
    "                theta_d = self.__sampling_theta(alpha=params)\n",
    "                # サンプリングされた値で更新\n",
    "                self.theta[d] = theta_d\n",
    "            \n",
    "            # 更新されたn_k_vから、φ_kをサンプリング\n",
    "            for k in range(self.n_topic):\n",
    "                params = self.n_k_v[k] + self.beta\n",
    "                phi_k = self.__sampling_phi(beta=params)\n",
    "                # サンプリングされた値で更新\n",
    "                self.phi[k] = phi_k\n",
    "            \n",
    "            # n_d_kのサンプル合計数をプラス\n",
    "            sum_n_d_k += self.n_d_k\n",
    "            # n_d_kのサンプル平均を計算\n",
    "            ave_n_d_k = sum_n_d_k / (s+1)\n",
    "            self.__update_dirichlet_param(ave_n_d_k)\n",
    "            \n",
    "            if (s+1) % self.PRINT_EVERYITER == 0:\n",
    "                print('{} iter finished !'.format(s+1))\n",
    "        return self\n",
    "    \n",
    "    def __predict_pdf(self, test_lda, d, v):\n",
    "        pdf = 0.0\n",
    "        \n",
    "        for k in range(self.n_topic):\n",
    "            pdf += (test_lda.theta[d,k] * self.phi[k,v])\n",
    "            \n",
    "        return pdf\n",
    "                    \n",
    "        \n",
    "    def perplexity(self, test1, test2):\n",
    "        \"\"\"\n",
    "        test1を用いてテスト用パラメータ学習\n",
    "        test2を用いてperplexityを計算\n",
    "        \n",
    "        @param test1 list perplexityの計算に用いるΘを学習するための文書集合\n",
    "        @param test2 list テストデータの集合\n",
    "        \n",
    "        @return perplexity float \n",
    "        \"\"\"\n",
    "        # perplexityを計算するためにΘ^(test)を学習\n",
    "        test_lda = LDA(self.n_iter, self.n_topic)\n",
    "        test_lda.fit(test1)\n",
    "        \n",
    "        likelihood = 0.0\n",
    "        n_words = 0\n",
    "        for d, doc in enumerate(test2):\n",
    "            for word in doc:\n",
    "                # 単語を番号に変化\n",
    "                v = self.vocab.get(word, False)\n",
    "                # 単語が辞書になかったら飛ばす\n",
    "                if not v:\n",
    "                    continue\n",
    "                likelihood += np.log(self.__predict_pdf(test_lda, d, v))\n",
    "                n_words += 1\n",
    "        return np.exp(-likelihood / n_words)\n",
    "    \n",
    "    def print_topn_pertopic(self, n=5):\n",
    "        index_to_words = {v: k for k, v in self.vocab.items()}\n",
    "        for k in range(self.n_topic):\n",
    "            print('-----topic {}-----'.format(k))\n",
    "            index_phi_k = self.phi[k].argsort()[::-1]\n",
    "            for print_num, v in enumerate(index_phi_k):\n",
    "                if print_num >= n:\n",
    "                    break\n",
    "                \n",
    "                print('{}, pdf:{}'.format(index_to_words[v],\n",
    "                                          self.phi[k, v]))\n",
    "                \n",
    "            \n",
    "\n",
    "\n",
    "def load_train_test():\n",
    "    \"\"\"\n",
    "    @return train list 学習用の文書集合\n",
    "    @return test list テスト用の文書集合\n",
    "    \"\"\"\n",
    "    read_dir = './data/ldcourpas/'\n",
    "    train_doc_name = 'train_doclist.list'\n",
    "    test_doc_name = 'test_doclist.list'\n",
    "    \n",
    "    with open(read_dir + train_doc_name, mode='rb') as f:\n",
    "        train = pickle.load(f)\n",
    "    with open(read_dir + test_doc_name, mode='rb') as f:\n",
    "        test = pickle.load(f)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "\n",
    "def split_testdata(test, ratio):\n",
    "    \"\"\"\n",
    "    @param ratio float testデータを ratio : (1 - ratio) に分割する\n",
    "    \n",
    "    @return test1 list perplexity計算の際にΘを求めるデータ\n",
    "    @return test2 list perplexity計算用のデータ\n",
    "    \"\"\"\n",
    "    test1 = []\n",
    "    test2 = []\n",
    "    \n",
    "    for doc in test:\n",
    "        n_words = len(doc)\n",
    "        # 小数点以下、四捨五入\n",
    "        index = int(round(n_words*ratio, 0))\n",
    "        test1.append(doc[:index])\n",
    "        test2.append(doc[index:])\n",
    "    \n",
    "    return test1, test2\n",
    "        \n",
    "\n",
    "def main():\n",
    "    train, test = load_train_test()\n",
    "    test1, test2 = split_testdata(test, ratio=.5)\n",
    "    \n",
    "    lda = LDA(n_iter=100, n_topic=10)\n",
    "    lda.fit(train)\n",
    "    with open('./lda.model', mode='wb') as f:\n",
    "        dill.dump(lda, f)\n",
    "    lda.perplexity(test1, test2)\n",
    "    lda.print_topn_pertopic(n=10)\n",
    "    \n",
    "    \n",
    "main()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gensimのLDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------topic 0--------\n",
      "0.014*\"映画\" + 0.010*\"作品\" + 0.009*\"公開\" + 0.009*\"月日\" + 0.007*\"ライブ\" + 0.006*\"今夏\" + 0.006*\"女性\" + 0.006*\"たち\" + 0.005*\"回\" + 0.005*\"歳\"\n",
      "--------topic 1--------\n",
      "0.035*\"充電\" + 0.027*\"更新\" + 0.017*\"事象\" + 0.010*\"端子\" + 0.007*\"S\" + 0.007*\"対応\" + 0.006*\"ACアダプタ\" + 0.006*\"円\" + 0.006*\"ケーブル\" + 0.006*\"台\"\n",
      "--------topic 2--------\n",
      "0.011*\"D\" + 0.007*\"撮影\" + 0.006*\"日本\" + 0.006*\"つとむ\" + 0.006*\"機能\" + 0.005*\"映画\" + 0.005*\"時間\" + 0.004*\"今夏\" + 0.004*\"アプリ\" + 0.004*\"ソフトウェア\"\n",
      "--------topic 3--------\n",
      "0.019*\"smartphone\" + 0.015*\"対応\" + 0.014*\"D\" + 0.014*\"発表\" + 0.013*\"MAX\" + 0.013*\"S\" + 0.013*\"NTTドコモ\" + 0.012*\"円\" + 0.010*\"利用\" + 0.010*\"向け\"\n",
      "--------topic 4--------\n",
      "0.034*\"独女\" + 0.014*\"歳\" + 0.013*\"アプリ\" + 0.012*\"友達\" + 0.011*\"さん\" + 0.009*\"とき\" + 0.008*\"彼\" + 0.007*\"男\" + 0.006*\"女性\" + 0.006*\"紹介\"\n",
      "--------topic 5--------\n",
      "0.025*\"更新\" + 0.016*\"搭載\" + 0.014*\"S\" + 0.013*\"smartphone\" + 0.012*\"対応\" + 0.012*\"画面\" + 0.010*\"表示\" + 0.010*\"機能\" + 0.008*\"利用\" + 0.008*\"ソフトウェア\"\n",
      "--------topic 6--------\n",
      "0.029*\"更新\" + 0.017*\"ソフトウェア\" + 0.013*\"機能\" + 0.011*\"当社\" + 0.010*\"場合\" + 0.009*\"コピー\" + 0.008*\"ダウンロード\" + 0.008*\"アプリ\" + 0.008*\"表示\" + 0.007*\"ROM\"\n",
      "--------topic 7--------\n",
      "0.026*\"さん\" + 0.023*\"結婚\" + 0.017*\"歳\" + 0.015*\"女性\" + 0.013*\"仕事\" + 0.011*\"私\" + 0.011*\"いい\" + 0.010*\"男性\" + 0.009*\"女\" + 0.009*\"気\"\n",
      "--------topic 8--------\n",
      "0.014*\"さん\" + 0.010*\"汗\" + 0.008*\"女性\" + 0.007*\"たち\" + 0.007*\"時\" + 0.006*\"話題\" + 0.005*\"位\" + 0.005*\"女\" + 0.004*\"機能\" + 0.004*\"私\"\n",
      "--------topic 9--------\n",
      "0.009*\"女性\" + 0.008*\"S\" + 0.007*\"さん\" + 0.006*\"簡易\" + 0.006*\"ライブ\" + 0.005*\"たち\" + 0.005*\"月日\" + 0.005*\"子供\" + 0.005*\"番組\" + 0.005*\"旅\"\n",
      "--------topic 10--------\n",
      "0.012*\"月日\" + 0.009*\"万\" + 0.008*\"発表\" + 0.007*\"memnck\" + 0.007*\"円\" + 0.007*\"発売\" + 0.007*\"話題\" + 0.005*\"開催\" + 0.005*\"約分\" + 0.005*\"球団\"\n",
      "--------topic 11--------\n",
      "0.019*\"さん\" + 0.012*\"氏\" + 0.011*\"選手\" + 0.007*\"歳\" + 0.007*\"男性\" + 0.006*\"それ\" + 0.006*\"監督\" + 0.006*\"時\" + 0.006*\"SportsWatch\" + 0.005*\"女性\"\n",
      "--------topic 12--------\n",
      "0.035*\"独女\" + 0.015*\"Twitter\" + 0.007*\"さん\" + 0.007*\"問題\" + 0.007*\"ゆるい\" + 0.007*\"情報\" + 0.006*\"tweet\" + 0.006*\"妊娠\" + 0.006*\"ソフトウェア\" + 0.006*\"機能\"\n",
      "--------topic 13--------\n",
      "0.013*\"歳\" + 0.010*\"女性\" + 0.010*\"男性\" + 0.009*\"さん\" + 0.009*\"仕事\" + 0.008*\"時\" + 0.008*\"いい\" + 0.007*\"彼\" + 0.007*\"とき\" + 0.007*\"既婚\"\n",
      "--------topic 14--------\n",
      "0.013*\"さん\" + 0.010*\"話\" + 0.009*\"歳\" + 0.007*\"たち\" + 0.006*\"それ\" + 0.006*\"私\" + 0.006*\"月日\" + 0.005*\"男性\" + 0.005*\"気\" + 0.004*\"映画\"\n",
      "--------topic 15--------\n",
      "0.019*\"さん\" + 0.013*\"女性\" + 0.010*\"男性\" + 0.008*\"いい\" + 0.008*\"なでしこジャパン\" + 0.007*\"話\" + 0.007*\"歳\" + 0.006*\"たち\" + 0.005*\"気\" + 0.005*\"今\"\n",
      "--------topic 16--------\n",
      "0.063*\"ソフトウェア\" + 0.019*\"更新\" + 0.012*\"D\" + 0.006*\"Xi\" + 0.006*\"S\" + 0.006*\"実施\" + 0.006*\"不具合\" + 0.005*\"月日\" + 0.005*\"NTTドコモ\" + 0.005*\"シリーズ\"\n",
      "--------topic 17--------\n",
      "0.015*\"肌\" + 0.012*\"memnck\" + 0.010*\"さん\" + 0.009*\"歳\" + 0.007*\"要件\" + 0.007*\"氏\" + 0.005*\"MAX\" + 0.005*\"天然\" + 0.005*\"女性\" + 0.004*\"独女\"\n",
      "--------topic 18--------\n",
      "0.018*\"アプリ\" + 0.018*\"MAX\" + 0.018*\"Android\" + 0.012*\"D\" + 0.012*\"S\" + 0.011*\"ソフトウェア\" + 0.011*\"smartphone\" + 0.011*\"画面\" + 0.010*\"対応\" + 0.009*\"エスマックス\"\n",
      "--------topic 19--------\n",
      "0.040*\"さん\" + 0.011*\"歳\" + 0.009*\"女性\" + 0.009*\"彼\" + 0.007*\"私\" + 0.006*\"時\" + 0.006*\"それ\" + 0.005*\"方\" + 0.005*\"男性\" + 0.005*\"いい\"\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "import pickle\n",
    "\n",
    "model_dir = './model/gensim/'\n",
    "with open('./data/ldcourpas/train_doclist.list', mode='rb') as f:\n",
    "    docs = pickle.load(f)\n",
    "dictionary = corpora.Dictionary(docs)\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.3)\n",
    "#dictionary.save_as_text(model_dir + 'dict.txt')\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "#corpora.MmCorpus.serialize(model_dir + 'cop.mm', corpus)\n",
    "\n",
    "n_topics = 20\n",
    "lda = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                      num_topics=n_topics,\n",
    "                                      id2word=dictionary)\n",
    "\n",
    "for i in range(n_topics):\n",
    "    print('--------topic {}--------'.format(i))\n",
    "    print(lda.print_topic(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## livedoorニュースコーパスをLDA用に変換\n",
    "\n",
    "データセットは[ここ](http://www.rondhuit.com/download.html#ldcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stopwordのリスト作成&読み込み\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['それ', 'さん', 'くん', 'そう', 'うん', 'あそこ', 'あたり', 'あちら', 'あっち', 'あと', 'あな', 'あなた', 'あれ', 'いくつ', 'いつ', 'いま', 'いや', 'いろいろ', 'うち', 'おおまか', 'おまえ', 'おれ', 'がい', 'かく', 'かたち', 'かやの', 'から', 'がら', 'きた', 'くせ', 'ここ', 'こっち', 'こと', 'ごと', 'こちら', 'ごっちゃ', 'これ', 'これら', 'ごろ', 'さまざま', 'さらい', 'さん', 'しかた', 'しよう', 'すか', 'ずつ', 'すね', 'すべて', 'ぜんぶ', 'そう', 'そこ', 'そちら', 'そっち', 'そで', 'それ', 'それぞれ', 'それなり', 'たくさん', 'たち', 'たび', 'ため', 'だめ', 'ちゃ', 'ちゃん', 'てん', 'とおり', 'とき', 'どこ', 'どこか', 'ところ', 'どちら', 'どっか', 'どっち', 'どれ', 'なか', 'なかば', 'なに', 'など', 'なん', 'はじめ', 'はず', 'はるか', 'ひと', 'ひとつ', 'ふく', 'ぶり', 'べつ', 'へん', 'ぺん', 'ほう', 'ほか', 'まさ', 'まし', 'まとも', 'まま', 'みたい', 'みつ', 'みなさん', 'みんな', 'もと', 'もの', 'もん', 'やつ', 'よう', 'よそ', 'わけ', 'わたし', 'ハイ', '上', '中', '下', '字', '年', '月', '日', '時', '分', '秒', '週', '火', '水', '木', '金', '土', '国', '都', '道', '府', '県', '市', '区', '町', '村', '各', '第', '方', '何', '的', '度', '文', '者', '性', '体', '人', '他', '今', '部', '課', '係', '外', '類', '達', '気', '室', '口', '誰', '用', '界', '会', '首', '男', '女', '別', '話', '私', '屋', '店', '家', '場', '等', '見', '際', '観', '段', '略', '例', '系', '論', '形', '間', '地', '員', '線', '点', '書', '品', '力', '法', '感', '作', '元', '手', '数', '彼', '彼女', '子', '内', '楽', '喜', '怒', '哀', '輪', '頃', '化', '境', '俺', '奴', '高', '校', '婦', '伸', '紀', '誌', 'レ', '行', '列', '事', '士', '台', '集', '様', '所', '歴', '器', '名', '情', '連', '毎', '式', '簿', '回', '匹', '個', '席', '束', '歳', '目', '通', '面', '円', '玉', '枚', '前', '後', '左', '右', '次', '先', '春', '夏', '秋', '冬', '一', '二', '三', '四', '五', '六', '七', '八', '九', '十', '百', '千', '万', '億', '兆', '下記', '上記', '時間', '今回', '前回', '場合', '一つ', '年生', '自分', 'ヶ所', 'ヵ所', 'カ所', '箇所', 'ヶ月', 'ヵ月', 'カ月', '箇月', '名前', '本当', '確か', '時点', '全部', '関係', '近く', '方法', '我々', '違い', '多く', '扱い', '新た', 'その後', '半ば', '結局', '様々', '以前', '以後', '以降', '未満', '以上', '以下', '幾つ', '毎日', '自体', '向こう', '何人', '手段', '同じ', '感じ']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "IN_DIR = './data/'\n",
    "TXT_FILE_NAME = 'Japanese.txt'\n",
    "OUT_DIR = './model/stopword'\n",
    "OUT_NAME = 'stopwords.list'\n",
    "\n",
    "def make_stop_wordslist():\n",
    "    stop_words = []\n",
    "    with open(IN_DIR + TXT_FILE_NAME, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line != '':\n",
    "                stop_words.append(line)\n",
    "    print(stop_words)\n",
    "    with open(OUT_DIR + OUT_NAME, 'wb') as f:\n",
    "        pickle.dump(stop_words, f)\n",
    "    \n",
    "def get_stop_words():\n",
    "    with open(OUT_DIR + OUT_NAME, 'rb') as f:\n",
    "        stop_words = pickle.load(f)\n",
    "    return stop_words\n",
    "\n",
    "make_stop_wordslist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文書を単語に分割する分割器\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "\n",
    "class Extractor(object):\n",
    "    INDEX_POS = 0\n",
    "    INDEX_BASE_FORM = 6\n",
    "    TARGET_POS = [\"名詞\", \" 動詞\",  \"形容詞\", \"感動詞\"]\n",
    "    def __init__(self):\n",
    "        neologd_path = '/usr/local/lib/mecab/dic/mecab-ipadic-neologd'\n",
    "        self.tagger = MeCab.Tagger('-o chasen -d ' + neologd_path)\n",
    "        self.stop_words = get_stop_words()\n",
    "        \n",
    "    def extract_words(self, sentence):\n",
    "        \"\"\"\n",
    "        日本語の文書を単語集合に変換（その際、単語は原型に直される）。\n",
    "        \n",
    "        @param text str 文書\n",
    "        @return words list 文書からターゲットの品詞の単語のみを抜き出したもの\n",
    "        \"\"\"\n",
    "        \n",
    "        if not sentence:\n",
    "            return []\n",
    "        \n",
    "        words = []\n",
    "        \n",
    "        node = self.tagger.parseToNode(sentence)\n",
    "        while node:\n",
    "            features = node.feature.split(',')\n",
    "            # 品詞がターゲットの品詞だった場合に処理\n",
    "            if features[self.INDEX_POS] in self.TARGET_POS:\n",
    "                # 原型がなかったら、表層系を保持\n",
    "                if features[self.INDEX_BASE_FORM] == '*':\n",
    "                    word = node.surface\n",
    "                else:\n",
    "                    word = features[self.INDEX_BASE_FORM].replace('。', '')\n",
    "                # 数字のみかどうか\n",
    "                only_digit = re.match(r'^[0-9]+$', word)\n",
    "                # ひらがな１文字のみかどうか\n",
    "                len_one = re.match(r'^[あ-ん]$', word)\n",
    "                if word not in self.stop_words and not only_digit and not len_one:\n",
    "                    words.append(word)\n",
    "            \n",
    "            node = node.next\n",
    "\n",
    "        return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "870\n",
      "------------------movie-enter-5847102.txt------------------\n",
      "['照英', 'カウボーイ&エイリアン', '宣伝', 'ナビゲーター', '就任', '涙', '感動', '10月22日', '公開', '映画', 'カウボーイ&エイリアン', '宣伝', 'ナビゲーター', 'タレント', '照英', '就任', '本作', '豪華', 'スタッフ', 'キャスト', 'SF', 'アクション', '大作', '物語', '舞台', '1873年', 'アリゾナ', '西部', '未知', '敵', '人々', '敵', '誰か', '記憶', '1人', 'カウボーイ', '巨大', '敵', '夜空', '腕輪', '青い', '閃光', '一体', 'カウボーイ', '何者', '巨大', '敵', '目的', '本作', 'アメリカ', '伝説', 'ビジュアルブック', 'COWBOY', 'ALIENS', '魅了', 'スティーヴン・スピルバーグ', '映画化', '実現', 'ロン・ハワード', 'アイアンマン', 'ジョン・ファブロー', '集結', '驚愕', '映像', '世界', '再現', 'キャスト', 'ダニエル・クレイグ', 'ハリソン・フォード', '豪華', '顔ぶれ', '勢揃い', '照英', '本作', '宣伝', 'ナビゲーター', '就任', '決定', 'インディ・ジョーンズ', 'E.T.', 'スピルバーグ', '映画', '僕', '涙', '興奮', '熱意', '最高', '本作', '魅力', '熱い', '宣伝', 'ナビゲーター', 'コメント', '今後', 'ない', 'smartphone', 'AR', '拡張現実', '活用', '本作', '魅力', '熱い', '予定', '現在', '一部', 'インターネット上', '照英', '画像', 'フレーズ', '絶大', '人気', '照英', '本作', 'ナビゲーター', '話題', '映画', 'カウボーイ&エイリアン', '10月22日', '丸の内ピカデリー', '全国ロードショー', 'カウボーイ&エイリアン', '公式サイト', 'カウボーイ&エイリアン', '作品情報', '関連', 'ニュース', 'カウボーイ&エイリアン', '画像', '独占', '入手', '青い', '奇妙', '金属', 'カウボーイ', '初', '試み', '映画', 'カウボーイ&エイリアン', 'チラシ', 'カウボーイ&エイリアン', '前売り券', '謎', '腕', '環', 'ゲット', 'スピルバーグ', '絶賛', 'コミック', '実写映画化', '注目', 'SF', 'アクション映画', 'カウボーイ&エイリアン']\n",
      "870\n",
      "------------------it-life-hack-6306547.txt------------------\n",
      "['3秒', 'イー・モバイル', '次世代', 'モバイル', '通信', 'EMOBILELTE', '対応', '端末', '一挙', '紹介', 'イー・アクセス', 'EMOBILELTE', '対応', '端末', 'Wi-Fiルータ', 'PocketWiFiLTE', '機種', 'USB', 'スティック', '機種', '2012年', '3月', '発売', '注目', '端末', '紹介', 'PocketWiFiLTEGL', 'P', 'PocketWiFiLTEGL', 'P', '容量', 'バッテリー', '搭載', 'Wi-Fiルータ', 'Pocket', 'WiFi', 'ベストセラー', 'モデル', 'デザイン', '踏襲', '薄', 'さと', '機能性', '両立', '連続', '通信', '9時間', '最大', 'Wi-Fi', '対応', '機器', '同時', '接続', '可能', 'PocketWiFiLTEGL', 'P', 'PocketWiFiLTEGL', 'P', '手のひら', 'コンパクト', 'サイズ', 'Wi-Fiルータ', '連続', '通信', '9時間', '最大', 'Wi-Fi', '対応', '機器', '同時', '接続', 'GL', 'D', 'GL', 'D', 'USB', 'スティック', 'タイプ', 'データ通信', '端末', '回転', 'USB', 'ケーブル', '使用', 'スペース', '使い勝手', 'よい', '実現', '端末', '価格', '発売日', 'お呼び', '予約', '開始', '決定', '次第', 'お知らせ', '最大', 'Mb', 'PS', '次世代', 'モバイル', '通信', 'EMOBILELTE', '3月', '提供開始', 'リリース', 'イー・アクセス', 'イー・モバイル', '関連', '記事', '国内', '初', 'XGA', 'タブレット端末', 'イー・モバイル', 'A', 'HW', '狙い', '発売', 'イー・モバイル', '最速', 'PocketWiFi', '機能', '発売日', '決定', '国内', '軽量', 'スマホ', 'PocketWiFiSII', 'S41', 'HW', '落とし穴', 'イーモバ', '通話', '定額', '無料', 'カラクリ', 'Transcend', 'SDHCカード', '16GB', 'Class', '永久', '保証', 'フラストレーション', 'フリー', 'パッケージ', 'FFPTS', 'GSDHC', 'E', 'トランセンド・ジャパン', '販売元', 'Amazon.co.jp', 'クチコミ']\n",
      "864\n",
      "------------------kaden-channel-5796317.txt------------------\n",
      "['女子', '必見', 'ソフトバンク', 'キティちゃん', 'スマホ', '登場', '小さ目', 'ピンク色', '種類', '豊富', '女子', '普及', 'smartphone', 'Hello Kitty', 'モチーフ', '機種', 'ソフトバンク', '登場', 'ソフトバンクモバイル', '発表', 'Hello Kitty', 'デザイン', '折りたたみ', '型', 'Android', '搭載', 'smartphone', '007SH', 'KT', 'シャープ', '製', '発売', 'AQUOSPHONETHEHYBRID', '007SH', 'ベース', 'モデル', '本体', '背面', '高級', 'Hello Kitty', 'Hello Kitty', 'デザイン', 'メニュー', '画面', '壁紙', 'デコレメール', '素材', '電卓', 'アラーム', 'オリジナル', 'コンテンツ', 'プリインストール', 'キティ', '好き', 'たまらない', '機種', '発売', '9月上旬', '予定', 'SPEC', 'OS', 'Android', '4インチ', 'New', 'モバイル', 'ASV', '液晶', '610万', '画素', 'カメラ', 'Bluetooth', 'Wi-Fi', 'microSDHC', 'カード', 'IP', 'X5', 'IPX', '相当', '防水', '性能', 'IP', 'X', '相当', '防塵', '性能', '1seg', 'おサイフケータイ', '赤外線', 'デコレメ', '緊急地震速報', '対応', 'カラー', 'アンティーク', 'ベリー', '1色', 'AQUOSPHONETHEHYBRID', '007SH', 'ソフトバンクモバイル', '関連', '記事', '真夏', 'オススメ・ホラーオペラ', 'BEST', 'DEJI', '話題', 'シニア', '世代', 'smartphone', '所有', '率', '%', 'PC', 'カメラ', '使用', '可能', '小型', 'トイデジカメ', 'GH', 'TCA', 'M30', 'P', '登場', 'おすすめ', 'Androidアプリ', 'ベスト5', '仕事', '効率化', '編', 'DEJI', '表面', '汚染', '検査', '計', 'ガイガーカウンター', '特集', '豆知識']\n",
      "770\n",
      "------------------topic-news-5927166.txt------------------\n",
      "['ゆうこりん', 'ハワイ', '結婚', '写真', '公開', 'タレント', '小倉優子', 'ヘア', 'メーク', 'アーティスト', '菊地勲', '挙式', '現地時間', '10日', '日本時間', '11日', '米', 'ハワイ', 'オアフ島', 'ウェディングドレス', '姿', '写真', '報道陣', '公開', 'ハワイ', '出発', '7日', '結婚会見', 'こりん星', '去年', '卒業', '普通', '嫁さん', 'コメント', '笑い', 'ゆうこりん', 'こりん星', 'りんご', 'ももか', '姫', 'お姫様', 'キャラクター', '過去', '関連', '記事', '小倉優子', '結婚会見', 'こりん星', '言及', '2011年', '10月']\n",
      "511\n",
      "------------------livedoor-homme-4632362.txt------------------\n",
      "['SHIPS', '伝授', 'ワン', 'ランク', 'スーツ', '新生活', '特集', 'スーツ', '音鼓-OTOKO-', '戦闘服', 'トレンド', 'スーツ', '着こなし', 'デキルオトコ', '近道', 'livedoor', 'HOMME', 'デキルオトコ', '新作', 'スーツ', '着こなし', '人気', 'セレクトショップ', 'スタッフ', '2010年', '最新', 'スーツ', '着こなし', 'インタビュー', 'SHIPS', '銀座', '土屋和之', '入門編', 'スーツ', '選び方', 'ワン', 'ランク', '着こなし', 'マスター', 'シャツ', 'ネクタイ', '靴', '費用対効果', '高位', '小物', 'レクチャー', 'ネクタイ', 'ラベンダー', 'スーツ', 'シャツ', 'ネクタイ', '選び方', '重要', 'オススメ', '組み合わせ', '土屋和之', 'ラベンダー', 'ネクタイ', 'オススメ', '白', 'ブルー', 'シャツ', 'やすい', 'ボタンダウン', 'カジュアル', 'シャツ', 'ニット', 'タイ', 'いい', 'フォーマル', 'シャツ', 'シルバー', 'サテン', '生地', 'いい', 'プロ', 'ならでは', 'ネクタイ', '結び方', '土屋', 'プレーンノット', '基本', '結び方', 'シャツ', '11円', 'タイ', '9円', 'SHIPS', 'シャツ', '12円', 'SHIPS', 'タイ', '13円', 'STEFANOBIGI', 'シャツ', '9円', 'SHIPS', 'タイ', '13円', 'STEFANOBIGI', 'シャツ', '12円', 'タイ', '9円', 'SHIPS', '問い合わせ', 'Cochira', 'スリッポン', '心地', 'いい', '靴', '小物', 'オススメ', '土屋', '紺', 'suede', 'スリッポン', '靴', '54円', 'クロケット', 'ジョーンズ', '雨', 'ラバーソール', 'いい', '靴', '36円', 'SHIPS', 'ジャケパン', '靴', '36円', 'SHIPS', '軽い', '生地', '最近', 'ビジネス', 'シーン', 'ジャケパン', '土屋', '年々', 'COOL BIZ', '浸透', '7月', '8月', 'スーツ', '紹介', 'スーツ', 'イタリア', '生地', 'シーズン', 'シリーズ', '一年', '着用', '生地', '厚い', '生地', 'スーツ', '84円', 'シャツ', '9円', 'チーフ', '3円', 'SHIPS', 'タイ', '9円', 'ベントレー', 'クラバッツ', '靴', '63円', 'クロケット', 'ジョーンズ', '問い合わせ', 'Cochira', '費用対効果', '高位', 'チーフ', '着こなし', 'ワンポイント', 'プラス', '土屋', 'チーフ', 'ビジネス', 'シーン', '無い', '値段', 'オリジナル', '3000円', '前後', '費用対効果', 'いい', 'ベーシック', '白', 'ラベンダー', 'いい', '柄物', '最初', '無地', '2枚', 'チェック', 'いい', 'ブルー', 'シルク', 'ドット', 'リネンパイピング', '3円', 'ラベンダー', 'シルク', '2円', 'SHIPS', '問い合わせ', 'Cochira', 'ポイント', '4つ', 'お話', 'ポイント', '土屋', 'ポイント', '4つ', '色', 'one tone', '腰', '周り', '肩', '周り', 'サイズ', '選び', 'スーツ', '価値', '生地', '素材', '流行', '小物', 'ありがとう', '深い', 'SHIPS', '銀座', '土屋', 'いい', '土屋', 'はい', '来店', 'お待ち', 'SHIPS', 'デキルオトコ', '新作', 'スーツ', '着こなし', '新生活', '特集', '新生活', '特集', 'SPRING', 'livedoor', 'HOMME', '関連リンク', 'SHIPS', '公式サイト', '問い合わせ', 'SHIPS', '銀座', '東京都中央区銀座', '−4', '−1', '菱', '進', '銀座', 'ビル', 'B1', '−3', 'F', 'Tel', '−3', '営業時間']\n",
      "842\n",
      "------------------peachy-4403717.txt------------------\n",
      "['鉄道', '好き', '女の子', '旅行', '15曲', '旅', '鉄道', '好き', '女の子', '結成', '鉄', '旅', 'てつ', 'GIRLS', '電車', '旅', '懐かしい', '旅', '気分', '曲', 'コンピレーション', 'CD', 'tabiuta', '旅', '歌', '鉄', '旅', 'GIRLS', 'recomend', '10月21日', '発売', '秋の', '旅行', 'シーズン', 'お供', '楽しい', 'センチメンタル', '選曲', '魅力的', '鉄', '旅', 'ニッポン', 'プロジェクト', 'プロジェクト', '開始', '若い', '女性', '鉄道', '旅', 'PR', '鉄', '旅', 'GIRLS', 'プロジェクト', '女子大生', '旅', '好き', '鉄道', '好き', '20代女性', '中心', 'メンバー', '鉄', '旅', 'GIRLS', '女の子', '女の子', '鉄', '旅', '情報発信', '活動', '鉄道', '旅', '楽しい', '旅', 'とも', '旅', '歌', 'テーマ', 'さすらい', '奥田民生', '赤い電車', '15曲', 'CD', '作成', '今後', 'グッズ', 'プラン', '制作', '予定', 'tabiuta', '旅', '歌', '鉄', '旅', 'GIRLS', 'recomend', 'さすらい', '奥田民生', 'そば', 'MAMALAIDRAG', 'feat', 'CHEMISTRY', '冨田ラボ', \"Let's\", 'GetTogetherNow', '川口大輔', '.S', 'halala', 'SkoopOnSomebody', '.S', 'OULS', 'B・I・R・D', '風をあつめて', 'Leyona', '赤い電車', '9.', '休日', 'ダイヤ', '麒麟児', 'JuniorSweet', 'CHARA', '雨', 'onetoomanyrain', 'ショコラ', '二人のアカボシ', 'キンモクセイ', '夜空ノムコウ', 'LiveVersion', '川村結花', 'いい日旅立ち・西へ', '鬼束ちひろ', 'LookBackAgain', '矢井田瞳', '関連リンク', '鉄', '旅', 'GIRLS', 'プロジェクト', 'tabiuta', '旅', '歌', '鉄', '旅', 'GIRLS', 'recomend', 'SonyMusicShop']\n",
      "900\n",
      "------------------sports-watch-4630515.txt------------------\n",
      "['SportsWatch', '浅田真央', '演技', '涙', '理由', 'バンクーバー五輪', '注目', '女子', 'フィギュアスケート', '日本テレビ', 'SUPER', 'うるぐす', '27日', '放送', '浅田真央', '涙', 'ワケ', 'SP', '決戦', '夜', '浅田真央', 'インタビュー', 'すごい', '嬉しい', '浅田', '前夜', '美味しい', '昨日', '寿司', 'バンクーバー', '人気', '真央', 'ロール', '持ち帰り', '笑顔', '番組', '用意', 'フリップ', 'ママ', '浅田', '感謝', '一人', 'ママ', 'ありがとう', '演技', '母', 'お疲れ様', '金メダル', 'じゃなくて', '嬉しい', '言葉', '浅田', '最愛', '母', '金メダル', 'プレゼント', '思い', '強い', 'インタビュアー', 'お母さん', '思い', 'はい', 'さっき', '笑顔', '一変', '静か', '涙']\n",
      "870\n",
      "------------------dokujo-tsushin-4809276.txt------------------\n",
      "['おばさん', 'おばさん', '同僚', '男性', '声', '顔', 'カオリ', '33歳', 'メーカー', '勤務', 'アルバイト', '大学生', '仕事', '説明', '同僚', '視線', '間違い', 'カオリ', '勢い', '雰囲気', '大学生', '会釈', 'カオリ', '世代', '同僚', '男性', 'おばさん', '納得', '納得', '出来事', '友人', '反応', '大学生', 'おばさん', '残念', 'ムキッ', 'イタイ', 'カオリ', '会釈', '大人', '対応', '同僚', '男性', '批判', '目板', '言葉', 'ない', 'カオリ', '慰め', '友人', '会社', 'プライベート', '体験', '子ども', '若い', 'おばさん', '少ない', '未婚女性', '30代', '40代', 'おばさん', '抵抗', '多い', 'おばさん', '言葉', '外見', '女性', '見た目', 'イメージ', '図々しい', '遠慮', 'ない', '行動', 'マイナスイメージ', 'おばさん', '風', 'おばさん', 'キャラ', '周囲', '良い', '人間関係', 'ハツミ', '38歳', '輸入', '関連', '勤務', '会社', '女性社員', '20代', '30代', 'ハツミ', '1人', '数年前', '20代', '一緒', '楽しい', '20代', 'キツ', 'くい', 'ハツミ', 'ハツミ', 'おばさん', '身だしなみ', 'アイドル', '話題', '素直', 'ねえねえ', '人気', 'ファッション', '女子', '遠慮ない', '着こなし', '図々しい', 'アレコレ', '楽しい', '上司', '遠慮ない', '無理', '若い', '職場', '人間関係', '上手', 'くい', 'ハツミ', 'おばさん', '言葉', '極度', 'マイナスイメージ', 'オバタリアン', '言葉', '登場', 'ころ', '1989年', '流行語大賞', '金賞', '受賞', '図々しい', '羞恥心', 'ない', '無神経', '自分勝手', '中年', '女性', '代表', '言葉', 'オバタリアン', '巷', 'おばさん', '定義', '部分', '大きい', '仕事', 'プライベート', '悪意', 'おばさん', '言葉', '無化', 'つき', '頭', '無理', 'マイナス', 'プラス', '鏡', 'チェック', 'おばさん', 'イメージ', '年齢', 'ファッション', '年齢', '行動', '心', '大丈夫', 'イメージチェンジ', '良い', 'きっかけ', '災い', '福', '余談', 'お菓子', 'ケーキ', 'ブランド', 'おばさん', 'クッキー', 'おばさん', 'チーズケーキ', 'ふくよか', '女性', 'キャラクター', 'やすい', 'お菓子', '美味しい', 'イメージ', '模試', 'おばさん', 'やすい', '優しい', '雰囲気', 'オフィスエムツー', '神田', 'はるひ']\n",
      "870\n",
      "------------------smax-6519676.txt------------------\n",
      "['かわいい', 'デコメ', 'Gmail', '簡単', 'Gmail', 'デコメ', 'デビュー', 'NetFront', 'Communicator', 'Androidアプリ', '予想', 'デコメ', '素材', '多い', 'キャリアメール', 'メール', 'スマホデビュー', 'きっかけ', 'Gmail', 'Yahoo', 'メール', 'Webメール', 'デビュー', '多い', '携帯電話', 'キャリア', 'アドレス', '変更', '連絡', 'メール', 'バックアップ', '作業', '面倒', 'Webメール', '手間', '便利', '一面', 'Gmail', 'Yahoo', 'メール', 'IMAP', '対応', '最大', '2つ', 'アカウント', 'グリッド', 'ツリー', '表示', 'INTERFACE', 'カスタマイズ', 'デコパーツ', 'テンプレート', '600点', '無料', 'Webメール', 'メール', 'デコ', '医方', 'デコメ', 'デビュー', '医方', 'オススメ', 'アプリ', 'NetFront', 'Communicator', '紹介', '既存', 'メール', 'アカウント', '入力', '設定', 'パソコン', '設定', 'Folder', '表示', 'うまい', '表示', '右下', '新着', '問合せ', 'タップ', 'アカウント', '登録', '画面', 'Gmail', '起動', '画面', 'Android', '端末', 'メニュー', '画面', 'メニュー', '表示', 'ツリー', '表示', 'タップ', 'グリッド', '表示', '切替', '好み', 'Android', '端末', 'メニュー', '押下', '画面', 'ツリー', '表示', '画面', 'iKON', '変更', 'メニュー', 'Folder', '編集', '変更', 'フォルダ', 'タップ', 'イラスト', '色', 'タップ', '変更', 'アプリ', 'Folder', '作成', '名称', '変更', '削除', 'パソコン', 'メール', 'ソフト', '変更', 'フォルダ', '画像', '色', '変更', '画面', '絵文字', 'デコパーツ', '装飾', 'やすい', '一覧', '表示', '好き', 'デコ', '絵文字', '絵文字', '行事', 'キャラクタ', 'チビデコ', 'デコピクチャ', 'ライン', '使用', '履歴', '装飾', 'DECO', 'メール', '送信', '楽しさ', '倍増', '友達', '同士', 'NetFront', 'Communicator', 'インストール', 'キャリアメール', 'テンプレート', '30種', 'テンプレート', 'デコメ', '楽しい', 'メール', 'デコアレンジ', '画面', 'テンプレート', '設定', 'プッシュ', '受信', '文字', 'サイズ', '引用', '返信', 'デザイン変更', 'メール', '通知', '音', 'アカウント', '削除', 'やすい', 'カスタマイズ', 'デザイン変更', 'お気に入り', '写真', '変更', '楽しさ', 'アップ', '設定', '画面', 'デザイン変更', '画面', '受信', 'メール', 'フラグ', 'Folder', '移動', '未読', '本文', '選択', 'コピー', '関連', 'メール', '表示', 'Gmail', '物足りない', 'オススメ', 'インストール', '可能', '端末', 'サポート', '注意', '記事', '執筆', 'にゃんこ', 'アプリ', 'NetFront', 'Communicator', '価格', '無料', 'カテゴリ', '通信', '開発者', 'ACCESSCO', 'LTD', 'バージョン', 'ANDROID', '要件', 'GooglePlayStore', 'HTTP', 'play', 'Google', '.com', 'store', 'apps', 'DETAILS', 'id', 'Com.A', 'ccess', 'company', 'ANDROID', 'nfcommunicator', '関連リンク', 'エスマックス', 'S', 'MAX']\n",
      "model saved\n",
      "model saved\n",
      "model saved\n",
      "model saved\n",
      "--------FINISH!!!--------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import mojimoji\n",
    "import codecs\n",
    "\n",
    "class CourpasReader(object):\n",
    "    ROOT_PATH = './data/text/'\n",
    "    DIR_NAMES = ['dokujo-tsushin',\n",
    "                 'kaden-channel',\n",
    "                 'movie-enter']\n",
    "    \n",
    "    DIR_NAMES = ['movie-enter',\n",
    "                 'it-life-hack',\n",
    "                 'kaden-channel',\n",
    "                 'topic-news',\n",
    "                 'livedoor-homme',\n",
    "                 'peachy',\n",
    "                 'sports-watch',\n",
    "                 'dokujo-tsushin',\n",
    "                 'smax']\n",
    "    \n",
    "    REMOVE_FILENAMES = 'LICENSE.txt'\n",
    "    \n",
    "    OUT_DIR = './data/ldcourpas'\n",
    "    NAME_TRAIN_DOC = 'train_doclist.list'\n",
    "    NAME_TRAIN_TITLE = 'train_titlelist.list'\n",
    "    NAME_TEST_DOC = 'test_doclist.list'\n",
    "    NAME_TEST_TITLE = 'test_titlelist.list'\n",
    "    \n",
    "    TRAIN_SIZE_PER_GENRE = 300\n",
    "    TEST_SIZE_PER_GENRE = 200\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.extractor = Extractor()\n",
    "    \n",
    "    def __digit_zen_to_han(self, text):\n",
    "        \"\"\"\n",
    "        テキスト内の全角数字を半角数字に変換\n",
    "        \n",
    "        @param text str\n",
    "        @return text_transformed str text内の全角数字を半角数字に変換したもの\n",
    "        \"\"\"\n",
    "        text_transfomed = text\n",
    "        match = re.findall(r'[０-９]+', text_transfomed)\n",
    "        for zen in match:\n",
    "            han = mojimoji.zen_to_han(zen)\n",
    "            text_transfomed = text_transfomed.replace(zen, han, 1)\n",
    "        return text_transfomed\n",
    "    \n",
    "    def __create_doc(self, path):\n",
    "        \"\"\"\n",
    "        @param path str 読み込む記事のパス\n",
    "        \n",
    "        @return doc list 記事を単語単位に分割したもの。\n",
    "        \"\"\"\n",
    "        \n",
    "        doc = []\n",
    "        \n",
    "        with codecs.open(path, 'r', 'utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                # 最初の2行はurlと時刻なのでカット\n",
    "                if i > 1:\n",
    "                    \"\"\"\n",
    "                    # 改行文字を「。」に置き換え\n",
    "                    line = re.sub(r'[\\n]', '。', line)\n",
    "                    # 「。」が2個以上連続してたら１つにする\n",
    "                    line = re.sub(r'[。]+', '。', line)\n",
    "                    # 全角特殊文字\n",
    "                    line = re.sub(r'[！？”“％＆＄＃（）［］]', '', line)\n",
    "                    \"\"\"\n",
    "                    line = line.encode('utf-8').decode('utf-8')\n",
    "                    line = line.strip()\n",
    "                    line = self.__digit_zen_to_han(line)\n",
    "                    line = re.sub(r'[\\s!\"#$%&()=^~{}\\[\\]+*;:@]', '', line)\n",
    "                    if line != '':\n",
    "                        words = self.extractor.extract_words(line)\n",
    "                        doc.extend(words)\n",
    "        return doc\n",
    "    \n",
    "    def __print_doc(self, path):\n",
    "        doc = ''\n",
    "        \n",
    "        with open(path, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                print(line)\n",
    "                \n",
    "                \n",
    "    def __save_obj(self, path, obj):\n",
    "        with open(path, mode='wb') as f:\n",
    "            pickle.dump(obj, f)\n",
    "        print('model saved')\n",
    "    \n",
    "    def extract_doclist(self):\n",
    "        \"\"\"\n",
    "        各ディレクトリから、文書名と文書を抜き出し学習データとテストデータを作成\n",
    "        \"\"\"\n",
    "        train_doclist = []\n",
    "        train_titlelist = []\n",
    "        test_doclist = []\n",
    "        test_titlelist = []\n",
    "        \n",
    "        for dir_name in self.DIR_NAMES:\n",
    "            dir_path = self.ROOT_PATH + dir_name\n",
    "            titles = os.listdir(dir_path)\n",
    "            titles.remove(self.REMOVE_FILENAMES)\n",
    "            print(len(titles))\n",
    "            end = self.TRAIN_SIZE_PER_GENRE + self.TEST_SIZE_PER_GENRE\n",
    "            for i, title in enumerate(sorted(titles[0:end])):\n",
    "                doc = self.__create_doc(dir_path + '/' + title)\n",
    "                \n",
    "                if i == 10:\n",
    "                    print('------------------{}------------------'\\\n",
    "                          .format(title))\n",
    "                    print(doc)\n",
    "                    \n",
    "                \n",
    "                if i < self.TRAIN_SIZE_PER_GENRE:\n",
    "                    train_doclist.append(doc)\n",
    "                    train_titlelist.append(title)\n",
    "                else:\n",
    "                    test_doclist.append(doc)\n",
    "                    test_titlelist.append(title)\n",
    "        \n",
    "        self.__save_obj(path=self.OUT_DIR + self.NAME_TRAIN_DOC,\n",
    "                        obj=train_doclist)\n",
    "        \n",
    "        self.__save_obj(path=self.OUT_DIR + self.NAME_TRAIN_TITLE,\n",
    "                        obj=train_titlelist)\n",
    "        \n",
    "        self.__save_obj(path=self.OUT_DIR + self.NAME_TEST_DOC,\n",
    "                        obj=test_doclist)\n",
    "        \n",
    "        self.__save_obj(path=self.OUT_DIR + self.NAME_TEST_TITLE,\n",
    "                        obj=test_titlelist)\n",
    "        print('--------FINISH!!!--------')\n",
    "\n",
    "        \n",
    "def main():\n",
    "    reader = CourpasReader()\n",
    "    reader.extract_doclist()\n",
    "    \n",
    "\n",
    "main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 映画レビューのデータセット\n",
    "\n",
    "IMDbWebサイトの映画レビューデータセット([ここ](http://ai.stanford.edu/~amaas/data/sentiment/)から入手できる)を利用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "import pickle\n",
    "\n",
    "reviews_train = load_files('data/aclImdb/train/')\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "text_train = [doc.replace(b'<br />', b'') for doc in text_train]\n",
    "\n",
    "reviews_test = load_files('data/aclImdb/test/')\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "text_test = [doc.replace(b'<br />', b'') for doc in text_test]\n",
    "\n",
    "imdb_dir = './data/imdb/'\n",
    "with open(imdb_dir + 'text_test.list', mode='wb') as f:\n",
    "    pickle.dump(text_test, f)\n",
    "with open(imdb_dir + 'y_test.list', mode='wb') as f:\n",
    "    pickle.dump(y_test, f)\n",
    "with open(imdb_dir + 'text_train', mode='wb') as f:\n",
    "    pickle.dump(text_train, f)\n",
    "with open(imdb_dir + 'y_train', mode='wb') as f:\n",
    "    pickle.dump(y_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pickle\n",
    "\n",
    "\n",
    "imdb_dir = './data/imdb/'\n",
    "with open(imdb_dir + 'text_test.list', mode='rb') as f:\n",
    "    text_test = pickle.load(f)\n",
    "with open(imdb_dir + 'y_test.list', mode='rb') as f:\n",
    "    y_test = pickle.load(f)\n",
    "with open(imdb_dir + 'text_train', mode='rb') as f:\n",
    "    text_train = pickle.load(f)\n",
    "with open(imdb_dir + 'y_train', mode='rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "    \n",
    "\n",
    "vect = CountVectorizer(max_features=10000, max_df=.15)\n",
    "X = vect.fit_transform(text_train[0:2000])\n",
    "\n",
    "lda_sklearn = LatentDirichletAllocation(n_components=10, learning_method='batch',\n",
    "                                max_iter=25, random_state=0)\n",
    "document_topics = lda_sklearn.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       topic 3       topic 4       \n",
      "--------      --------      --------      --------      --------      \n",
      "scenes        performance   show          book          play          \n",
      "worst         new           star          nothing       through       \n",
      "lot           fact          episode       didn          big           \n",
      "another       while         luke          give          must          \n",
      "look          actor         family        enough        cast          \n",
      "video         old           original      ve            school        \n",
      "minutes       find          own           original      woman         \n",
      "although      role          part          read          world         \n",
      "pretty        always        scenes        work          dominick      \n",
      "same          young         look          through       awful         \n",
      "\n",
      "\n",
      "topic 5       topic 6       topic 7       topic 8       topic 9       \n",
      "--------      --------      --------      --------      --------      \n",
      "match         while         young         ve            funny         \n",
      "steve         re            horror        funny         disney        \n",
      "show          new           show          every         own           \n",
      "going         show          michael       real          show          \n",
      "now           us            years         show          comedy        \n",
      "saw           men           now           series        cast          \n",
      "while         real          music         actors        family        \n",
      "comedy        old           around        thing         young         \n",
      "horror        find          seems         scenes        work          \n",
      "rather        through       cast          through       actors        \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorting = np.argsort(lda_sklearn.components_, axis=1)[:, ::-1]\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "mglearn.tools.print_topics(topics=range(10), feature_names=feature_names,\n",
    "                          sorting=sorting, topics_per_chunk=5, n_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished making vocab\n",
      "start iterating\n",
      "1 iter finished !\n",
      "2 iter finished !\n",
      "3 iter finished !\n",
      "4 iter finished !\n",
      "5 iter finished !\n",
      "6 iter finished !\n",
      "7 iter finished !\n",
      "8 iter finished !\n",
      "9 iter finished !\n",
      "10 iter finished !\n",
      "11 iter finished !\n",
      "12 iter finished !\n",
      "13 iter finished !\n",
      "14 iter finished !\n",
      "15 iter finished !\n",
      "16 iter finished !\n",
      "17 iter finished !\n",
      "18 iter finished !\n",
      "19 iter finished !\n",
      "20 iter finished !\n",
      "21 iter finished !\n",
      "22 iter finished !\n",
      "23 iter finished !\n",
      "24 iter finished !\n",
      "25 iter finished !\n",
      "26 iter finished !\n",
      "27 iter finished !\n",
      "28 iter finished !\n",
      "29 iter finished !\n",
      "30 iter finished !\n",
      "31 iter finished !\n",
      "32 iter finished !\n",
      "33 iter finished !\n",
      "34 iter finished !\n",
      "35 iter finished !\n",
      "36 iter finished !\n",
      "37 iter finished !\n",
      "38 iter finished !\n",
      "39 iter finished !\n",
      "40 iter finished !\n",
      "41 iter finished !\n",
      "42 iter finished !\n",
      "43 iter finished !\n",
      "44 iter finished !\n",
      "45 iter finished !\n",
      "46 iter finished !\n",
      "47 iter finished !\n",
      "48 iter finished !\n",
      "49 iter finished !\n",
      "50 iter finished !\n",
      "51 iter finished !\n",
      "52 iter finished !\n",
      "53 iter finished !\n",
      "54 iter finished !\n",
      "55 iter finished !\n",
      "56 iter finished !\n",
      "57 iter finished !\n",
      "58 iter finished !\n",
      "59 iter finished !\n",
      "60 iter finished !\n",
      "61 iter finished !\n",
      "62 iter finished !\n",
      "63 iter finished !\n",
      "64 iter finished !\n",
      "65 iter finished !\n",
      "66 iter finished !\n",
      "67 iter finished !\n",
      "68 iter finished !\n",
      "69 iter finished !\n",
      "70 iter finished !\n",
      "71 iter finished !\n",
      "72 iter finished !\n",
      "73 iter finished !\n",
      "74 iter finished !\n",
      "75 iter finished !\n",
      "76 iter finished !\n",
      "77 iter finished !\n",
      "78 iter finished !\n",
      "79 iter finished !\n",
      "80 iter finished !\n",
      "81 iter finished !\n",
      "82 iter finished !\n",
      "83 iter finished !\n",
      "84 iter finished !\n",
      "85 iter finished !\n",
      "86 iter finished !\n",
      "87 iter finished !\n",
      "88 iter finished !\n",
      "89 iter finished !\n",
      "90 iter finished !\n",
      "91 iter finished !\n",
      "92 iter finished !\n",
      "93 iter finished !\n",
      "94 iter finished !\n",
      "95 iter finished !\n",
      "96 iter finished !\n",
      "97 iter finished !\n",
      "98 iter finished !\n",
      "99 iter finished !\n",
      "100 iter finished !\n",
      "101 iter finished !\n",
      "102 iter finished !\n",
      "103 iter finished !\n",
      "104 iter finished !\n",
      "105 iter finished !\n",
      "106 iter finished !\n",
      "107 iter finished !\n",
      "108 iter finished !\n",
      "109 iter finished !\n",
      "110 iter finished !\n",
      "111 iter finished !\n",
      "112 iter finished !\n",
      "113 iter finished !\n",
      "114 iter finished !\n",
      "115 iter finished !\n",
      "116 iter finished !\n",
      "117 iter finished !\n",
      "118 iter finished !\n",
      "119 iter finished !\n",
      "120 iter finished !\n",
      "121 iter finished !\n",
      "122 iter finished !\n",
      "123 iter finished !\n",
      "124 iter finished !\n",
      "125 iter finished !\n",
      "126 iter finished !\n",
      "127 iter finished !\n",
      "128 iter finished !\n",
      "129 iter finished !\n",
      "130 iter finished !\n",
      "131 iter finished !\n",
      "132 iter finished !\n",
      "133 iter finished !\n",
      "134 iter finished !\n",
      "135 iter finished !\n",
      "136 iter finished !\n",
      "137 iter finished !\n",
      "138 iter finished !\n",
      "139 iter finished !\n",
      "140 iter finished !\n",
      "141 iter finished !\n",
      "142 iter finished !\n",
      "143 iter finished !\n",
      "144 iter finished !\n",
      "145 iter finished !\n",
      "146 iter finished !\n",
      "147 iter finished !\n",
      "148 iter finished !\n",
      "149 iter finished !\n",
      "150 iter finished !\n",
      "151 iter finished !\n",
      "152 iter finished !\n",
      "153 iter finished !\n",
      "154 iter finished !\n",
      "155 iter finished !\n",
      "156 iter finished !\n",
      "157 iter finished !\n",
      "158 iter finished !\n",
      "159 iter finished !\n",
      "160 iter finished !\n",
      "161 iter finished !\n",
      "162 iter finished !\n",
      "163 iter finished !\n",
      "164 iter finished !\n",
      "165 iter finished !\n",
      "166 iter finished !\n",
      "167 iter finished !\n",
      "168 iter finished !\n",
      "169 iter finished !\n",
      "170 iter finished !\n",
      "171 iter finished !\n",
      "172 iter finished !\n",
      "173 iter finished !\n",
      "174 iter finished !\n",
      "175 iter finished !\n",
      "176 iter finished !\n",
      "177 iter finished !\n",
      "178 iter finished !\n",
      "179 iter finished !\n",
      "180 iter finished !\n",
      "181 iter finished !\n",
      "182 iter finished !\n",
      "183 iter finished !\n",
      "184 iter finished !\n",
      "185 iter finished !\n",
      "186 iter finished !\n",
      "187 iter finished !\n",
      "188 iter finished !\n",
      "189 iter finished !\n",
      "190 iter finished !\n",
      "191 iter finished !\n",
      "192 iter finished !\n",
      "193 iter finished !\n",
      "194 iter finished !\n",
      "195 iter finished !\n",
      "196 iter finished !\n",
      "197 iter finished !\n",
      "198 iter finished !\n",
      "199 iter finished !\n",
      "200 iter finished !\n",
      "201 iter finished !\n",
      "202 iter finished !\n",
      "203 iter finished !\n",
      "204 iter finished !\n",
      "205 iter finished !\n",
      "206 iter finished !\n",
      "207 iter finished !\n",
      "208 iter finished !\n",
      "209 iter finished !\n",
      "210 iter finished !\n",
      "211 iter finished !\n",
      "212 iter finished !\n",
      "213 iter finished !\n",
      "214 iter finished !\n",
      "215 iter finished !\n",
      "216 iter finished !\n",
      "217 iter finished !\n",
      "218 iter finished !\n",
      "219 iter finished !\n",
      "220 iter finished !\n",
      "221 iter finished !\n",
      "222 iter finished !\n",
      "223 iter finished !\n",
      "224 iter finished !\n",
      "225 iter finished !\n",
      "226 iter finished !\n",
      "227 iter finished !\n",
      "228 iter finished !\n",
      "229 iter finished !\n",
      "230 iter finished !\n",
      "231 iter finished !\n",
      "232 iter finished !\n",
      "233 iter finished !\n",
      "234 iter finished !\n",
      "235 iter finished !\n",
      "236 iter finished !\n",
      "237 iter finished !\n",
      "238 iter finished !\n",
      "239 iter finished !\n",
      "240 iter finished !\n",
      "241 iter finished !\n",
      "242 iter finished !\n",
      "243 iter finished !\n",
      "244 iter finished !\n",
      "245 iter finished !\n",
      "246 iter finished !\n",
      "247 iter finished !\n",
      "248 iter finished !\n",
      "249 iter finished !\n",
      "250 iter finished !\n",
      "-----topic 0-----\n",
      "school, pdf:0.010392341297238527\n",
      "anything, pdf:0.009196266500014956\n",
      "down, pdf:0.00876070480385666\n",
      "ll, pdf:0.008338130798102594\n",
      "high, pdf:0.008148711366119731\n",
      "old, pdf:0.007958866724819168\n",
      "else, pdf:0.007640743900193343\n",
      "pretty, pdf:0.0075744528310175314\n",
      "shot, pdf:0.006943927586296163\n",
      "half, pdf:0.00631519963459748\n",
      "-----topic 1-----\n",
      "actors, pdf:0.011770334285600719\n",
      "same, pdf:0.00839922138935371\n",
      "another, pdf:0.00816785465379919\n",
      "come, pdf:0.008034333959057196\n",
      "want, pdf:0.007951990874654548\n",
      "part, pdf:0.0070599490100998\n",
      "performance, pdf:0.007011621456348117\n",
      "book, pdf:0.006838815738510508\n",
      "thought, pdf:0.006737718813556429\n",
      "again, pdf:0.006516815197775836\n",
      "-----topic 2-----\n",
      "family, pdf:0.016829087117781364\n",
      "john, pdf:0.014685305058184678\n",
      "young, pdf:0.010910137393863272\n",
      "old, pdf:0.009049158502311899\n",
      "through, pdf:0.007176235568896504\n",
      "horror, pdf:0.006849427908397877\n",
      "ve, pdf:0.006395701543887506\n",
      "plays, pdf:0.006213432069965486\n",
      "town, pdf:0.005615112427719022\n",
      "however, pdf:0.0053938214972475106\n",
      "-----topic 3-----\n",
      "show, pdf:0.017998155328781785\n",
      "re, pdf:0.008098724311017266\n",
      "script, pdf:0.007791394545857945\n",
      "while, pdf:0.007712672025583208\n",
      "now, pdf:0.007649306176794125\n",
      "instead, pdf:0.0073469239210326096\n",
      "audience, pdf:0.0072472876119263445\n",
      "take, pdf:0.006469195964412634\n",
      "going, pdf:0.0062183639018924056\n",
      "quite, pdf:0.0057871795851248495\n",
      "-----topic 4-----\n",
      "years, pdf:0.01025780103482702\n",
      "real, pdf:0.009656508312926582\n",
      "us, pdf:0.009476897930371755\n",
      "father, pdf:0.009057803473114976\n",
      "wife, pdf:0.007437286670424905\n",
      "girl, pdf:0.006936290962587126\n",
      "our, pdf:0.006824400028570107\n",
      "between, pdf:0.006621144874045786\n",
      "while, pdf:0.006595792407502268\n",
      "dead, pdf:0.006116481783608717\n",
      "-----topic 5-----\n",
      "work, pdf:0.011599237981361471\n",
      "look, pdf:0.007891510461306293\n",
      "real, pdf:0.006901252124159368\n",
      "men, pdf:0.006888210217343178\n",
      "interesting, pdf:0.006669444233988167\n",
      "perhaps, pdf:0.00662195759410737\n",
      "far, pdf:0.006500994886295128\n",
      "fun, pdf:0.006422768544429241\n",
      "overall, pdf:0.006139365214606771\n",
      "true, pdf:0.0061180843620305975\n",
      "-----topic 6-----\n",
      "world, pdf:0.010488199943941881\n",
      "between, pdf:0.009577570164194004\n",
      "new, pdf:0.009334921384999617\n",
      "role, pdf:0.00833504450522494\n",
      "yet, pdf:0.007147839929198739\n",
      "american, pdf:0.0063649545524784275\n",
      "michael, pdf:0.006293629236310368\n",
      "plays, pdf:0.0060554390640057375\n",
      "us, pdf:0.005752422028747618\n",
      "might, pdf:0.005495235181594568\n",
      "-----topic 7-----\n",
      "fact, pdf:0.011362806279335348\n",
      "however, pdf:0.009733832756305202\n",
      "original, pdf:0.009571626286789875\n",
      "own, pdf:0.009237488895748169\n",
      "both, pdf:0.008410425414353783\n",
      "cast, pdf:0.00821419961434268\n",
      "nothing, pdf:0.007099640998243064\n",
      "full, pdf:0.00628774212848447\n",
      "along, pdf:0.005925826178712391\n",
      "special, pdf:0.005912955798592057\n",
      "-----topic 8-----\n",
      "funny, pdf:0.01303919714706262\n",
      "series, pdf:0.011855057382210817\n",
      "action, pdf:0.01090572784803605\n",
      "re, pdf:0.008769713020441973\n",
      "episode, pdf:0.008364007641434176\n",
      "now, pdf:0.008155336471754234\n",
      "goes, pdf:0.00782579049890044\n",
      "gets, pdf:0.0074656789281446236\n",
      "quite, pdf:0.007319459552435053\n",
      "fun, pdf:0.006910282332648786\n",
      "-----topic 9-----\n",
      "ve, pdf:0.013031836184732314\n",
      "lot, pdf:0.012007513406613761\n",
      "got, pdf:0.00890028266577597\n",
      "music, pdf:0.008266177674842785\n",
      "didn, pdf:0.007983662189300774\n",
      "10, pdf:0.0072728338990441675\n",
      "guy, pdf:0.007260101131300677\n",
      "worst, pdf:0.006962748581704344\n",
      "saw, pdf:0.006944820876904609\n",
      "through, pdf:0.006782134176872196\n"
     ]
    }
   ],
   "source": [
    "lda = LDA(n_iter=250, n_topic=10, max_df=.15, lang='en')\n",
    "lda.fit(text_train[0:2000])\n",
    "lda.print_topn_pertopic(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['２１', '４００', '２１', '２', '１', '２０１０', '３０', '２９', '４００']\n",
      "Forever21、来店者数400万人突破のファストファッションチェーンForever21本社米国ロサンゼルス2日日本直営1号店原宿店2010年1月30日昨年4月29日延べ来店客数400万人突破\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import mojimoji\n",
    "\n",
    "text = 'Forever２１、来店者数４００万人突破のファストファッションチェーン'\\\n",
    "        'Forever２１本社米国ロサンゼルス２日日本直営１号店原宿店２０１０年1月３０日'\\\n",
    "        '昨年4月２９日延べ来店客数４００万人突破'\n",
    "match = re.findall(r'[０-９]+', text)\n",
    "print(match)\n",
    "for zen in match:\n",
    "    han = mojimoji.zen_to_han(zen)\n",
    "    text = text.replace(zen, han, 1)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-11-17 12:46:11--  http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt\n",
      "Resolving svn.sourceforge.jp... 202.221.179.25\n",
      "Connecting to svn.sourceforge.jp|202.221.179.25|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2202 (2.2K) [text/plain]\n",
      "Saving to: ‘Japanese.txt’\n",
      "\n",
      "Japanese.txt        100%[===================>]   2.15K  --.-KB/s    in 0s      \n",
      "\n",
      "2017-11-17 12:46:11 (131 MB/s) - ‘Japanese.txt’ saved [2202/2202]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA.ipynb            \u001b[34mdata\u001b[m\u001b[m                 \u001b[34mmodel\u001b[m\u001b[m\r\n",
      "\u001b[34mcache\u001b[m\u001b[m                ldcc-20140209.tar.gz sampling_test.ipynb\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
