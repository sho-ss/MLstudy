{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-681b6fc036da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-681b6fc036da>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m     \u001b[0mtest1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_testdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-681b6fc036da>\u001b[0m in \u001b[0;36mload_train_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrain_doc_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtest_doc_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "import scipy.special as special\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import dill\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "class LDA(object):\n",
    "    PRINT_EVERYITER = 1\n",
    "    \n",
    "    def __init__(self, n_iter, n_topic, max_df=0.3, min_df=0.01, lang='ja'):\n",
    "        self.n_iter = n_iter\n",
    "        self.n_topic = n_topic\n",
    "        self.max_df = max_df\n",
    "        self.min_df = min_df\n",
    "        self.topics = [i for i in range(self.n_topic)]\n",
    "        self.alpha = None\n",
    "        self.beta = None\n",
    "        self.lang = lang\n",
    "        \n",
    "    def __sampling_theta(self, alpha):\n",
    "        return stats.dirichlet.rvs(alpha=alpha)[0]\n",
    "    \n",
    "    def __sampling_phi(self, beta):\n",
    "        return stats.dirichlet.rvs(alpha=beta)[0]\n",
    "    \n",
    "    def __sampling_z(self, d, v):\n",
    "        \"\"\"\n",
    "        @param d int 文章番号\n",
    "        @param v int 文章dのi番目の単語の単語辞書での番号\n",
    "        \n",
    "        @return k int 単語vのトピック\n",
    "        \"\"\"\n",
    "        theta_d = self.theta[d]\n",
    "        phi_v = self.phi[:, v]\n",
    "        sum_weight = np.dot(theta_d, phi_v.reshape(-1, 1))\n",
    "        weight = theta_d*phi_v / sum_weight\n",
    "        \n",
    "        return np.random.choice(self.topics, size=1, p=weight)\n",
    "    \n",
    "    def __init_dirichlet_param(self):\n",
    "        self.alpha = np.array([10 / self.n_topic\n",
    "                               for _ in range(self.n_topic)])\n",
    "        self.beta = np.array([100 / self.vocab_size\n",
    "                              for _ in range(self.vocab_size)])\n",
    "    \n",
    "    def __update_dirichlet_param(self, ave_n_d_k):\n",
    "        \"\"\"\n",
    "        update alpha\n",
    "        \n",
    "        @param ave_n_d_k ndarray n_d_kのサンプル平均\n",
    "        \"\"\"\n",
    "        alpha_sum = np.sum(self.alpha)\n",
    "        \n",
    "        # alphaのupdate\n",
    "        for k in range(self.n_topic):\n",
    "            alpha_k = self.alpha[k]\n",
    "            \n",
    "            a_k_d = [(special.psi(ave_n_d_k[d, k] + alpha_k)\\\n",
    "                      - special.psi(alpha_k)) * alpha_k\n",
    "                     for d in range(self.n_docs)]\n",
    "            \n",
    "            b_d = [special.psi(np.sum(ave_n_d_k[:,k]) + alpha_sum)\\\n",
    "                   - special.psi(alpha_sum)\n",
    "                   for _ in range(self.n_docs)]\n",
    "            \n",
    "            self.alpha[k] = np.sum(a_k_d) / np.sum(b_d)\n",
    "    \n",
    "    def __make_vocab(self, X):\n",
    "        \"\"\"\n",
    "        文書集合から単語辞書を作成し、文書をbag-of-wordsに変換\n",
    "        \n",
    "        @param X list 文書集合 要素は、各文章を単語に分割したもの\n",
    "        \n",
    "        @return vocab dict 文書内の単語辞書\n",
    "        @return X_bow list 文書をbag-of-wordsで表現した文書集合\n",
    "        \"\"\"\n",
    "        if self.lang == 'ja':\n",
    "            # 文書は既に単語の集合に変換されているので、splitする必要がない。\n",
    "            analyzer = lambda words: words\n",
    "            self.vect = CountVectorizer(min_df=self.min_df,\n",
    "                                        max_df=self.max_df,\n",
    "                                        analyzer=analyzer)\n",
    "        else:\n",
    "            self.vect = CountVectorizer(max_features=10000,\n",
    "                                        max_df=self.max_df)\n",
    "        X_bow = self.vect.fit_transform(X).toarray()\n",
    "        return self.vect.vocabulary_, X_bow\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        self.vocabを求める関数未実装\n",
    "        \n",
    "        @param X list 文書集合\n",
    "        @return self 学習済みモデル\n",
    "        \"\"\"\n",
    "        # 辞書作成と文書をbag-of-wordsに変換\n",
    "        self.vocab, X_bow = self.__make_vocab(X)\n",
    "        print('finished making vocab')\n",
    "        \n",
    "        self.n_docs = len(X_bow)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "        self.__init_dirichlet_param()\n",
    "        \n",
    "        # θとφの初期化\n",
    "        # theta ndarray theta[d,k] 文章dにトピックkが出現する確率を保持\n",
    "        self.theta = np.array([self.__sampling_theta(alpha=self.alpha)\n",
    "                               for _ in range(self.n_docs)])\n",
    "        \n",
    "        # phi ndarray phi[k,v] トピックkに単語vが含まれている確率を保持\n",
    "        self.phi = np.array([self.__sampling_phi(beta=self.beta)\n",
    "                             for _ in range(self.n_topic)])\n",
    "        \n",
    "        # n_d_k ndarray n_d_k[d,k] 文章d出現したトピックkの数\n",
    "        self.n_d_k = np.zeros((self.n_docs, self.n_topic))\n",
    "        \n",
    "        # n_k_v ndarray n_k_b[k,v] トピックがkの単語vの個数\n",
    "        self.n_k_v = np.zeros((self.n_topic, self.vocab_size))\n",
    "        \n",
    "        # n_d_kのこれまでのサンプル合計\n",
    "        sum_n_d_k = np.zeros((self.n_docs, self.n_topic))\n",
    "        \n",
    "        print('start iterating')\n",
    "        for s in range(self.n_iter):\n",
    "            # 初期化\n",
    "            self.n_d_k.fill(0.0)\n",
    "            self.n_k_v.fill(0.0)\n",
    "            \n",
    "            start = time.time()\n",
    "            for d in range(self.n_docs):\n",
    "                for v, n_word in enumerate(X_bow[d]):\n",
    "                    if n_word ==0:\n",
    "                        continue\n",
    "                    \n",
    "                    # 単語d,i=vのトピック\n",
    "                    k = self.__sampling_z(d, v)\n",
    "                    \n",
    "                    # それぞれの個数をプラス1\n",
    "                    self.n_k_v[k, v] += n_word\n",
    "                    self.n_d_k[d, k] += n_word\n",
    "                \n",
    "                # 更新されたn_d_kから、θ_dをサンプリング\n",
    "                params = self.n_d_k[d] + self.alpha\n",
    "                theta_d = self.__sampling_theta(alpha=params)\n",
    "                # サンプリングされた値で更新\n",
    "                self.theta[d] = theta_d\n",
    "            \n",
    "            # 更新されたn_k_vから、φ_kをサンプリング\n",
    "            for k in range(self.n_topic):\n",
    "                params = self.n_k_v[k] + self.beta\n",
    "                phi_k = self.__sampling_phi(beta=params)\n",
    "                # サンプリングされた値で更新\n",
    "                self.phi[k] = phi_k\n",
    "            \n",
    "            # n_d_kのサンプル合計数をプラス\n",
    "            sum_n_d_k += self.n_d_k\n",
    "            # n_d_kのサンプル平均を計算\n",
    "            ave_n_d_k = sum_n_d_k / (s+1)\n",
    "            self.__update_dirichlet_param(ave_n_d_k)\n",
    "            \n",
    "            elapsed_time = time.time() - start\n",
    "            if (s+1) % self.PRINT_EVERYITER == 0:\n",
    "                print('elapsed time: {} [sec]'.format(elapsed_time))\n",
    "                print('{} iter finished !'.format(s+1))\n",
    "        return self\n",
    "    \n",
    "    def __predict_pdf(self, test_lda, d, v):\n",
    "        pdf = 0.0\n",
    "        \n",
    "        for k in range(self.n_topic):\n",
    "            pdf += (test_lda.theta[d,k] * self.phi[k,v])\n",
    "            \n",
    "        return pdf\n",
    "                    \n",
    "        \n",
    "    def perplexity(self, test1, test2):\n",
    "        \"\"\"\n",
    "        test1を用いてテスト用パラメータ学習\n",
    "        test2を用いてperplexityを計算\n",
    "        \n",
    "        @param test1 list perplexityの計算に用いるΘを学習するための文書集合\n",
    "        @param test2 list テストデータの集合\n",
    "        \n",
    "        @return perplexity float \n",
    "        \"\"\"\n",
    "        # perplexityを計算するためにΘ^(test)を学習\n",
    "        test_lda = LDA(self.n_iter, self.n_topic)\n",
    "        test_lda.fit(test1)\n",
    "        \n",
    "        likelihood = 0.0\n",
    "        n_words = 0\n",
    "        for d, doc in enumerate(test2):\n",
    "            for word in doc:\n",
    "                # 単語を番号に変化\n",
    "                v = self.vocab.get(word, False)\n",
    "                # 単語が辞書になかったら飛ばす\n",
    "                if not v:\n",
    "                    continue\n",
    "                likelihood += np.log(self.__predict_pdf(test_lda, d, v))\n",
    "                n_words += 1\n",
    "        return np.exp(-likelihood / n_words)\n",
    "    \n",
    "    def print_topn_pertopic(self, n=5):\n",
    "        index_to_words = {v: k for k, v in self.vocab.items()}\n",
    "        for k in range(self.n_topic):\n",
    "            print('-----topic {}-----'.format(k))\n",
    "            index_phi_k = self.phi[k].argsort()[::-1]\n",
    "            for print_num, v in enumerate(index_phi_k):\n",
    "                if print_num >= n:\n",
    "                    break\n",
    "                \n",
    "                print('{}, pdf:{}'.format(index_to_words[v],\n",
    "                                          self.phi[k, v]))\n",
    "                \n",
    "            \n",
    "\n",
    "\n",
    "def load_train_test():\n",
    "    \"\"\"\n",
    "    @return train list 学習用の文書集合\n",
    "    @return test list テスト用の文書集合\n",
    "    \"\"\"\n",
    "    read_dir = './data/ldcourpas/'\n",
    "    train_doc_name = 'train_doclist.list'\n",
    "    test_doc_name = 'test_doclist.list'\n",
    "    \n",
    "    with open(read_dir + train_doc_name, mode='rb') as f:\n",
    "        train = pickle.load(f)\n",
    "    with open(read_dir + test_doc_name, mode='rb') as f:\n",
    "        test = pickle.load(f)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "\n",
    "def split_testdata(test, ratio):\n",
    "    \"\"\"\n",
    "    @param ratio float testデータを ratio : (1 - ratio) に分割する\n",
    "    \n",
    "    @return test1 list perplexity計算の際にΘを求めるデータ\n",
    "    @return test2 list perplexity計算用のデータ\n",
    "    \"\"\"\n",
    "    test1 = []\n",
    "    test2 = []\n",
    "    \n",
    "    for doc in test:\n",
    "        n_words = len(doc)\n",
    "        # 小数点以下、四捨五入\n",
    "        index = int(round(n_words*ratio, 0))\n",
    "        test1.append(doc[:index])\n",
    "        test2.append(doc[index:])\n",
    "    \n",
    "    return test1, test2\n",
    "        \n",
    "\n",
    "def main():\n",
    "    train, test = load_train_test()\n",
    "    #test1, test2 = split_testdata(test, ratio=.5)\n",
    "    \n",
    "    lda = LDA(n_iter=100, n_topic=10)\n",
    "    lda.fit(train)\n",
    "    with open('./lda.model', mode='wb') as f:\n",
    "        dill.dump(lda, f)\n",
    "    #lda.perplexity(test1, test2)\n",
    "    #lda.print_topn_pertopic(n=10)\n",
    "    \n",
    "    \n",
    "main()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gensimのLDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------topic 0--------\n",
      "0.014*\"映画\" + 0.010*\"作品\" + 0.009*\"公開\" + 0.009*\"月日\" + 0.007*\"ライブ\" + 0.006*\"今夏\" + 0.006*\"女性\" + 0.006*\"たち\" + 0.005*\"回\" + 0.005*\"歳\"\n",
      "--------topic 1--------\n",
      "0.035*\"充電\" + 0.027*\"更新\" + 0.017*\"事象\" + 0.010*\"端子\" + 0.007*\"S\" + 0.007*\"対応\" + 0.006*\"ACアダプタ\" + 0.006*\"円\" + 0.006*\"ケーブル\" + 0.006*\"台\"\n",
      "--------topic 2--------\n",
      "0.011*\"D\" + 0.007*\"撮影\" + 0.006*\"日本\" + 0.006*\"つとむ\" + 0.006*\"機能\" + 0.005*\"映画\" + 0.005*\"時間\" + 0.004*\"今夏\" + 0.004*\"アプリ\" + 0.004*\"ソフトウェア\"\n",
      "--------topic 3--------\n",
      "0.019*\"smartphone\" + 0.015*\"対応\" + 0.014*\"D\" + 0.014*\"発表\" + 0.013*\"MAX\" + 0.013*\"S\" + 0.013*\"NTTドコモ\" + 0.012*\"円\" + 0.010*\"利用\" + 0.010*\"向け\"\n",
      "--------topic 4--------\n",
      "0.034*\"独女\" + 0.014*\"歳\" + 0.013*\"アプリ\" + 0.012*\"友達\" + 0.011*\"さん\" + 0.009*\"とき\" + 0.008*\"彼\" + 0.007*\"男\" + 0.006*\"女性\" + 0.006*\"紹介\"\n",
      "--------topic 5--------\n",
      "0.025*\"更新\" + 0.016*\"搭載\" + 0.014*\"S\" + 0.013*\"smartphone\" + 0.012*\"対応\" + 0.012*\"画面\" + 0.010*\"表示\" + 0.010*\"機能\" + 0.008*\"利用\" + 0.008*\"ソフトウェア\"\n",
      "--------topic 6--------\n",
      "0.029*\"更新\" + 0.017*\"ソフトウェア\" + 0.013*\"機能\" + 0.011*\"当社\" + 0.010*\"場合\" + 0.009*\"コピー\" + 0.008*\"ダウンロード\" + 0.008*\"アプリ\" + 0.008*\"表示\" + 0.007*\"ROM\"\n",
      "--------topic 7--------\n",
      "0.026*\"さん\" + 0.023*\"結婚\" + 0.017*\"歳\" + 0.015*\"女性\" + 0.013*\"仕事\" + 0.011*\"私\" + 0.011*\"いい\" + 0.010*\"男性\" + 0.009*\"女\" + 0.009*\"気\"\n",
      "--------topic 8--------\n",
      "0.014*\"さん\" + 0.010*\"汗\" + 0.008*\"女性\" + 0.007*\"たち\" + 0.007*\"時\" + 0.006*\"話題\" + 0.005*\"位\" + 0.005*\"女\" + 0.004*\"機能\" + 0.004*\"私\"\n",
      "--------topic 9--------\n",
      "0.009*\"女性\" + 0.008*\"S\" + 0.007*\"さん\" + 0.006*\"簡易\" + 0.006*\"ライブ\" + 0.005*\"たち\" + 0.005*\"月日\" + 0.005*\"子供\" + 0.005*\"番組\" + 0.005*\"旅\"\n",
      "--------topic 10--------\n",
      "0.012*\"月日\" + 0.009*\"万\" + 0.008*\"発表\" + 0.007*\"memnck\" + 0.007*\"円\" + 0.007*\"発売\" + 0.007*\"話題\" + 0.005*\"開催\" + 0.005*\"約分\" + 0.005*\"球団\"\n",
      "--------topic 11--------\n",
      "0.019*\"さん\" + 0.012*\"氏\" + 0.011*\"選手\" + 0.007*\"歳\" + 0.007*\"男性\" + 0.006*\"それ\" + 0.006*\"監督\" + 0.006*\"時\" + 0.006*\"SportsWatch\" + 0.005*\"女性\"\n",
      "--------topic 12--------\n",
      "0.035*\"独女\" + 0.015*\"Twitter\" + 0.007*\"さん\" + 0.007*\"問題\" + 0.007*\"ゆるい\" + 0.007*\"情報\" + 0.006*\"tweet\" + 0.006*\"妊娠\" + 0.006*\"ソフトウェア\" + 0.006*\"機能\"\n",
      "--------topic 13--------\n",
      "0.013*\"歳\" + 0.010*\"女性\" + 0.010*\"男性\" + 0.009*\"さん\" + 0.009*\"仕事\" + 0.008*\"時\" + 0.008*\"いい\" + 0.007*\"彼\" + 0.007*\"とき\" + 0.007*\"既婚\"\n",
      "--------topic 14--------\n",
      "0.013*\"さん\" + 0.010*\"話\" + 0.009*\"歳\" + 0.007*\"たち\" + 0.006*\"それ\" + 0.006*\"私\" + 0.006*\"月日\" + 0.005*\"男性\" + 0.005*\"気\" + 0.004*\"映画\"\n",
      "--------topic 15--------\n",
      "0.019*\"さん\" + 0.013*\"女性\" + 0.010*\"男性\" + 0.008*\"いい\" + 0.008*\"なでしこジャパン\" + 0.007*\"話\" + 0.007*\"歳\" + 0.006*\"たち\" + 0.005*\"気\" + 0.005*\"今\"\n",
      "--------topic 16--------\n",
      "0.063*\"ソフトウェア\" + 0.019*\"更新\" + 0.012*\"D\" + 0.006*\"Xi\" + 0.006*\"S\" + 0.006*\"実施\" + 0.006*\"不具合\" + 0.005*\"月日\" + 0.005*\"NTTドコモ\" + 0.005*\"シリーズ\"\n",
      "--------topic 17--------\n",
      "0.015*\"肌\" + 0.012*\"memnck\" + 0.010*\"さん\" + 0.009*\"歳\" + 0.007*\"要件\" + 0.007*\"氏\" + 0.005*\"MAX\" + 0.005*\"天然\" + 0.005*\"女性\" + 0.004*\"独女\"\n",
      "--------topic 18--------\n",
      "0.018*\"アプリ\" + 0.018*\"MAX\" + 0.018*\"Android\" + 0.012*\"D\" + 0.012*\"S\" + 0.011*\"ソフトウェア\" + 0.011*\"smartphone\" + 0.011*\"画面\" + 0.010*\"対応\" + 0.009*\"エスマックス\"\n",
      "--------topic 19--------\n",
      "0.040*\"さん\" + 0.011*\"歳\" + 0.009*\"女性\" + 0.009*\"彼\" + 0.007*\"私\" + 0.006*\"時\" + 0.006*\"それ\" + 0.005*\"方\" + 0.005*\"男性\" + 0.005*\"いい\"\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "import pickle\n",
    "\n",
    "model_dir = './model/gensim/'\n",
    "with open('./data/ldcourpas/train_doclist.list', mode='rb') as f:\n",
    "    docs = pickle.load(f)\n",
    "dictionary = corpora.Dictionary(docs)\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.3)\n",
    "#dictionary.save_as_text(model_dir + 'dict.txt')\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "#corpora.MmCorpus.serialize(model_dir + 'cop.mm', corpus)\n",
    "\n",
    "n_topics = 20\n",
    "lda = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                      num_topics=n_topics,\n",
    "                                      id2word=dictionary)\n",
    "\n",
    "for i in range(n_topics):\n",
    "    print('--------topic {}--------'.format(i))\n",
    "    print(lda.print_topic(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## livedoorニュースコーパスをLDA用に変換\n",
    "\n",
    "データセットは[ここ](http://www.rondhuit.com/download.html#ldcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stopwordのリスト作成&読み込み\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['あそこ', 'あたり', 'あちら', 'あっち', 'あと', 'あな', 'あなた', 'あれ', 'いくつ', 'いつ', 'いま', 'いや', 'いろいろ', 'うち', 'おおまか', 'おまえ', 'おれ', 'がい', 'かく', 'かたち', 'かやの', 'から', 'がら', 'きた', 'くせ', 'くん', 'ここ', 'こっち', 'こと', 'ごと', 'こちら', 'ごっちゃ', 'これ', 'これら', 'ごろ', 'さまざま', 'さらい', 'さん', 'しかた', 'しよう', 'すか', 'ずつ', 'すね', 'すべて', 'ぜんぶ', 'そう', 'そこ', 'そちら', 'そっち', 'そで', 'それ', 'それぞれ', 'それなり', 'たくさん', 'たち', 'たび', 'ため', 'だめ', 'ちゃ', 'ちゃん', 'てん', 'とおり', 'とき', 'どこ', 'どこか', 'ところ', 'どちら', 'どっか', 'どっち', 'どれ', 'なか', 'なかば', 'なに', 'など', 'なん', 'はじめ', 'はず', 'はるか', 'ひと', 'ひとつ', 'ふく', 'ぶり', 'べつ', 'へん', 'ぺん', 'ほう', 'ほか', 'まさ', 'まし', 'まとも', 'まま', 'みたい', 'みつ', 'みなさん', 'みんな', 'もと', 'もの', 'もん', 'やつ', 'よう', 'よそ', 'わけ', 'わたし', 'ハイ', '上', '中', '下', '字', '年', '月', '日', '時', '分', '秒', '週', '火', '水', '木', '金', '土', '国', '都', '道', '府', '県', '市', '区', '町', '村', '各', '第', '方', '何', '的', '度', '文', '者', '性', '体', '人', '他', '今', '部', '課', '係', '外', '類', '達', '気', '室', '口', '誰', '用', '界', '会', '首', '男', '女', '別', '話', '私', '屋', '店', '家', '場', '等', '見', '際', '観', '段', '略', '例', '系', '論', '形', '間', '地', '員', '線', '点', '書', '品', '力', '法', '感', '作', '元', '手', '数', '彼', '彼女', '子', '内', '楽', '喜', '怒', '哀', '輪', '頃', '化', '境', '俺', '奴', '高', '校', '婦', '伸', '紀', '誌', 'レ', '行', '列', '事', '士', '台', '集', '様', '所', '歴', '器', '名', '情', '連', '毎', '式', '簿', '回', '匹', '個', '席', '束', '歳', '目', '通', '面', '円', '玉', '枚', '前', '後', '左', '右', '次', '先', '春', '夏', '秋', '冬', '一', '二', '三', '四', '五', '六', '七', '八', '九', '十', '百', '千', '万', '億', '兆', '下記', '上記', '時間', '今回', '前回', '場合', '一つ', '年生', '自分', 'ヶ所', 'ヵ所', 'カ所', '箇所', 'ヶ月', 'ヵ月', 'カ月', '箇月', '名前', '本当', '確か', '時点', '全部', '関係', '近く', '方法', '我々', '違い', '多く', '扱い', '新た', 'その後', '半ば', '結局', '様々', '以前', '以後', '以降', '未満', '以上', '以下', '幾つ', '毎日', '自体', '向こう', '何人', '手段', '同じ', '感じ']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "IN_DIR = './data/'\n",
    "TXT_FILE_NAME = 'Japanese.txt'\n",
    "OUT_DIR = './model/stopword'\n",
    "OUT_NAME = 'stopwords.list'\n",
    "\n",
    "def make_stop_wordslist():\n",
    "    stop_words = []\n",
    "    with open(IN_DIR + TXT_FILE_NAME, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line != '':\n",
    "                stop_words.append(line)\n",
    "    print(stop_words)\n",
    "    with open(OUT_DIR + OUT_NAME, 'wb') as f:\n",
    "        pickle.dump(stop_words, f)\n",
    "    \n",
    "def get_stop_words():\n",
    "    with open(OUT_DIR + OUT_NAME, 'rb') as f:\n",
    "        stop_words = pickle.load(f)\n",
    "    return stop_words\n",
    "\n",
    "make_stop_wordslist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文書を単語に分割する分割器\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "\n",
    "class Extractor(object):\n",
    "    INDEX_POS = 0\n",
    "    INDEX_BASE_FORM = 6\n",
    "    TARGET_POS = [\"名詞\", \" 動詞\",  \"形容詞\", \"感動詞\"]\n",
    "    def __init__(self):\n",
    "        neologd_path = '/usr/local/lib/mecab/dic/mecab-ipadic-neologd'\n",
    "        self.tagger = MeCab.Tagger('-o chasen -d ' + neologd_path)\n",
    "        self.stop_words = get_stop_words()\n",
    "        \n",
    "    def extract_words(self, sentence):\n",
    "        \"\"\"\n",
    "        日本語の文書を単語集合に変換（その際、単語は原型に直される）。\n",
    "        \n",
    "        @param text str 文書\n",
    "        @return words list 文書からターゲットの品詞の単語のみを抜き出したもの\n",
    "        \"\"\"\n",
    "        \n",
    "        if not sentence:\n",
    "            return []\n",
    "        \n",
    "        words = []\n",
    "        \n",
    "        node = self.tagger.parseToNode(sentence)\n",
    "        while node:\n",
    "            features = node.feature.split(',')\n",
    "            # 品詞がターゲットの品詞だった場合に処理\n",
    "            if features[self.INDEX_POS] in self.TARGET_POS:\n",
    "                # 原型がなかったら、表層系を保持\n",
    "                if features[self.INDEX_BASE_FORM] == '*':\n",
    "                    word = node.surface\n",
    "                else:\n",
    "                    word = features[self.INDEX_BASE_FORM].replace('。', '')\n",
    "                # 数字のみかどうか\n",
    "                only_digit = re.match(r'^[0-9]+$', word)\n",
    "                # ひらがな１文字のみかどうか\n",
    "                len_one = re.match(r'^[あ-ん]$', word)\n",
    "                if word not in self.stop_words and not only_digit and not len_one:\n",
    "                    words.append(word)\n",
    "            \n",
    "            node = node.next\n",
    "\n",
    "        return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "870\n",
      "------------------movie-enter-5847102.txt------------------\n",
      "['照英', 'カウボーイ&エイリアン', '宣伝', 'ナビゲーター', '就任', '涙', '感動', '10月22日', '公開', '映画', 'カウボーイ&エイリアン', '宣伝', 'ナビゲーター', 'タレント', '照英', '就任', '本作', '豪華', 'スタッフ', 'キャスト', 'SF', 'アクション', '大作', '物語', '舞台', '1873年', 'アリゾナ', '西部', '未知', '敵', '人々', '敵', '誰か', '記憶', '1人', 'カウボーイ', '巨大', '敵', '夜空', '腕輪', '青い', '閃光', '一体', 'カウボーイ', '何者', '巨大', '敵', '目的', '本作', 'アメリカ', '伝説', 'ビジュアルブック', 'COWBOY', 'ALIENS', '魅了', 'スティーヴン・スピルバーグ', '映画化', '実現', 'ロン・ハワード', 'アイアンマン', 'ジョン・ファブロー', '集結', '驚愕', '映像', '世界', '再現', 'キャスト', 'ダニエル・クレイグ', 'ハリソン・フォード', '豪華', '顔ぶれ', '勢揃い', '照英', '本作', '宣伝', 'ナビゲーター', '就任', '決定', 'インディ・ジョーンズ', 'E.T.', 'スピルバーグ', '映画', '僕', '涙', '興奮', '熱意', '最高', '本作', '魅力', '熱い', '宣伝', 'ナビゲーター', 'コメント', '今後', 'ない', 'smartphone', 'AR', '拡張現実', '活用', '本作', '魅力', '熱い', '予定', '現在', '一部', 'インターネット上', '照英', '画像', 'フレーズ', '絶大', '人気', '照英', '本作', 'ナビゲーター', '話題', '映画', 'カウボーイ&エイリアン', '10月22日', '丸の内ピカデリー', '全国ロードショー', 'カウボーイ&エイリアン', '公式サイト', 'カウボーイ&エイリアン', '作品情報', '関連', 'ニュース', 'カウボーイ&エイリアン', '画像', '独占', '入手', '青い', '奇妙', '金属', 'カウボーイ', '初', '試み', '映画', 'カウボーイ&エイリアン', 'チラシ', 'カウボーイ&エイリアン', '前売り券', '謎', '腕', '環', 'ゲット', 'スピルバーグ', '絶賛', 'コミック', '実写映画化', '注目', 'SF', 'アクション映画', 'カウボーイ&エイリアン']\n",
      "870\n",
      "------------------it-life-hack-6301650.txt------------------\n",
      "['プレミアム', 'Ultrabook', '登場', 'ガラス', '多用', 'ノートパソコン', 'DEJI', '日本HP', 'Ultrabook', 'モデル', 'プレミアム', 'Ultrabook', 'HPEnvy', 'Spectre', '発表', 'HP', 'Ultrabook', 'Folio', '素材', 'デザイン', 'プレミアム', '製品', 'HP', 'デザイナー', 'コラボ', 'ノートパソコン', 'VivienneTamEdition', 'リリース', '同様', 'プレミアム', '製品', 'HPEnvy', 'Spectre', 'HP', '個人向け', 'リーズナブル', 'ノートパソコン', 'Pavilion', 'シリーズ', '上位モデル', 'Envy', 'シリーズ', 'Envy', 'シリーズ', 'SPEC', '高い', '質感', '高い', '製品', 'Envy', 'シリーズ', 'プレミアム', 'Ultrabook', '登場', 'プレミアム', 'Ultrabook', '通常', 'Ultrabook', 'プレミアム', 'インテル', 'HP', '出し手', 'カテゴリ', '製品', 'Spectre', '英語', 'SPECTER', '日本', '語源', 'ラテン語', 'スペクトル', 'SPECTRUM', '読み方', '採用', '製品', '読み方', 'エイチピー・エンビー・フォーティーン・スペクトル', 'パームレスト', 'ガラス', '製', 'Beatsaudio', '対応', 'ガラス', '多用', '筐体', '製品', '特徴', 'ガラス', '多様', '液晶', '画面', '天板', 'パームレス', 'ガラス', '使用', '金属', 'プラスチック', '高級', '筐体', 'ガラス', 'コーニング', 'ゴリラガラス', '簡単', '傷', '無い', '製品', '型', '液晶', 'パネル', '採用', '一般', 'Ultrabook', '型', '回り', '画面', '大きい', '解像度', '液晶', '情報量', '多い', '液晶', '大きい', '狭い', '額縁', '仕様', '本体サイズ', '型', 'Folio', '同等', 'キーボード', 'LED', 'ライト', 'ユーザー', '近接', 'センサー', '付き', 'キー', '文字', 'タイプ', '高級感', '高い', '店頭', 'モデル', '128GB', 'SSD', '5万円', '程度', 'HPDirectplus', 'モデル', '256GB', 'SSD', '16万円', '前後', '発売', '3月中旬', 'プレミアム', 'Ultrabook', '登場', 'Ultrabook', '展開', '面白い', '日本HP', '上倉', '賢', 'kamikuradigi', 'DEJI', 'digi', 'デジタル', '現在', 'デジタル', '機器', '難しい', '皆さん', 'デジタル', '機器', '情報', '皆さん', '少し', '執筆', '陣', '提供', 'IT', 'life hack', 'Twitter', 'DEJI', '記事', '高品質', \"Let's Note\", '工場', '開発', '現場', '並み', '検査', '施設', 'サービス', '終了', '電子書籍', '永遠', 'インテル', 'SSD', 'Mac', '装着', '旧式', 'Mac', '高速', 'OS', '使い勝手', 'ハイブリッド', 'PC', '今後', 'キヤノン', '純正', 'インク', 'キャノンインクタンク', '5色', 'マルチ', 'パック', 'BCI', '−3', 'MP', 'キヤノン', '販売元', 'Amazon.co.jp', 'クチコミ']\n",
      "864\n",
      "------------------kaden-channel-5789971.txt------------------\n",
      "['読書', '洋書', '好き', 'Kindle', 'オススメ', '理由', 'DEJI', '暑い', '今日この頃', '動き', 'つらい', '出版社', 'CM', 'ない', '読書', '僕', 'Kindle', '国際版', '予約', '期待', '通り', '僕', '読書', '環境', '革新的', 'Amazon', '電子ブックリーダー', 'Kindle', '日本', '便利', '存在', '一条真人', '日本', '入手', '悪い', '洋書', '日本国内', '洋書', '入手', '悪い', '期待', '三省堂', '大型書店', 'ない', '面倒', '価格', '高い', '問題', 'Amazon', '洋書', '緩和', '在庫', '翌日', 'デリバリー', 'ありがたい', '本', '価格', 'リーズナブル', '言うまでもない', 'Amazon', 'Kindle', '販売', '会社', '3つ', '国際版', 'Kindle', '現在', '国際版', 'Kindle', '2世代', '製品', '3つ', 'タイプ', '1つ', '6インチ', 'ディスプレイ', '3G', '通信', 'WiFi', '対応', '1つ', '同軸', '6インチ', 'ディスプレイ', 'WiFi', '通信', '対応', '9.', '7インチ', '液晶', '搭載', '3G', '通信', '可能', 'タイプ', 'KindleDX', '画面', '読書', '9.', '7インチ', 'いい', '携帯', '普通', '6インチ', 'オススメ', 'Kindle', 'ベストセラー', '3G', '通信', '搭載', '大きい', '3G', '版', '絶対', 'オススメ', '僕', '第2世代', '機', '6インチ', '3G', 'タイプ', 'タイプ', 'ディスプレイ', '電子ペーパー', '長時間', '読書', '問題', '秒殺', 'ダウンロード', '可能', '3G', '通信', '3G', '通信', '搭載', 'ユーザー', '電波', '本', '冊', '本', 'ダウンロード', '通常', '10秒', '程度', '凄い', 'ユーザー', '3G', '通信', '料', '必要', 'ない', '現実', '本体価格', '本', '価格', 'Kindle', '本体', '現在', '6インチ', '3G', 'モデル', '現在', 'ドル', 'レート', '5000円', '程度', 'お買い得', '価格', '満足', 'Kindle', 'Shop', '本', 'Kindle', '本体', 'アクセス', 'Kindle', 'Shop', '購入', '最新', 'ベストセラー', 'タイトル', '10ドル', '前半', '購入', '現在', '冊', 'ストア', '登録', '売れ筋', '電子ブック', '普通', '欲しい', '本', '国際版', 'Kindle', '洋書', '好きな人', '絶対', 'オススメ', 'Kindle', '現在', 'Amazon', '日本', 'サイト', '正規', '代理', '販売', '米国', 'サイト', '購入', '配送', 'リーズナブル', '洋書', '好きな人', '英語', 'サイト', '購入', '問題', '関連', '記事', '真夏', 'オススメ', 'ホラー映画', 'BEST', 'DEJI', 'おすすめ', 'Androidアプリ', 'ベスト5', '日常', '編', 'Android', 'スマホ', 'ゲリラ豪雨', 'パリ', '普及', 'しない', 'スマートTV', '一条真人', 'ichijomasahitodigi', 'DEJI', 'digi', 'デジタル', '現在', 'デジタル', '機器', '難しい', '皆さん', 'デジタル', '機器', '情報', '皆さん', '少し', '執筆', '陣', '提供', '関連', '記事', '特集', 'パナソニック', 'beauty', '家電', '個人', '被ばく', '線量計', '機能', 'お浚い', 'ガイガーカウンター', '特集', '豆知識', 'Google', 'モトローラ', '買収', 'Android', '市場', '変質', 'DEJI', '女子', '必見', 'ソフトバンク', 'キティちゃん', 'スマホ', '登場', '線量計', '本来', '機能', 'お浚い', 'ガイガーカウンター', '特集', '豆知識']\n",
      "770\n",
      "------------------topic-news-5918064.txt------------------\n",
      "['みのもんた', '引退', '芸能界', '勢力図', '東スポ', '林家三平', '国分佐智子', '披露宴', '出席', 'ほろ酔い', '加減', 'みのもんた', '直撃', 'とんでも', '言葉', 'みのもんた', '引退', '真実味', '東スポ', '独占', '直撃', '激', '白', 'HTTP', 'news', 'livedoor', '.com', 'article', 'DETAIL', '引退', '危機', 'みのもんた', '視聴率', '不振', 'TBS', '一日中', 'みの', 'HTTP', 'news', 'livedoor', '.com', 'article', 'DETAIL', '全局', '契約', '終了', '言葉', '思い', '東スポ', '報道', 'TV', '局', '事務所', '混乱', 'みのもんた', '引退', '騒動', '事務所', 'TV', '各局', '大慌て', 'HTTP', 'news', 'livedoor', '.com', 'article', 'DETAIL', 'みのもんた', '引退', '引退後', '動き', 'みのもんた', '引退', '騒動', '真相', 'キャスター', '学校', 'HTTP', 'news', 'livedoor', '.com', 'article', 'DETAIL', '視聴率男', 'みのもんた', '引退', '報道', '暴露', '出版', 'HTTP', 'news', 'livedoor', '.com', 'article', 'DETAIL', '暴露本', 'キャスター', '学校', 'いずれ', '実現', 'TV', '相当', '話題', '島田紳助', '芸能界引退', 'みのもんた', '引退', '司会者', '世代交代', '本格', 'みのもんた', '引退', '宣言', '大物', '司会者', '世代交代', '本格化', 'HTTP', 'news', 'livedoor', '.com', 'article', 'DETAIL', '来年', '芸能界', 'みのもんた', '島田紳助', 'ポジション', '獲得']\n",
      "842\n",
      "------------------peachy-4352879.txt------------------\n",
      "['運命の人', '発', '検索', 'あなたと', '相性', '良い', '生年月日', '運命', '相手', '運命の人', '存在', '20代', '30代', '独身', '男女', '4人', '1人', '結婚', '会員', '720万人', 'ネットメディア', 'girlswalker', '.com', '運営', '株式会社', 'ブランディング', '新サービス', '運命', '検索', '開始', '運命', '検索', '生年月日', '相手', '条件', '理想', '相手', '年齢', '3つ', '入力', '相性', '良い', '相手', '生年月日', '検索', '可能', 'どの日', '相性', '良い', '運命の人', '生年月日', 'なのか', '一目', 'サービス', '婚活', '時代', '指標', '運命', '検索', 'いかが', '1億', '恋愛', '白書', '運命', '検索', '対応機種', 'DoCoMo', 'au']\n",
      "900\n",
      "------------------sports-watch-4630515.txt------------------\n",
      "['SportsWatch', '浅田真央', '演技', '涙', '理由', 'バンクーバー五輪', '注目', '女子', 'フィギュアスケート', '日本テレビ', 'SUPER', 'うるぐす', '27日', '放送', '浅田真央', '涙', 'ワケ', 'SP', '決戦', '夜', '浅田真央', 'インタビュー', 'すごい', '嬉しい', '浅田', '前夜', '美味しい', '昨日', '寿司', 'バンクーバー', '人気', '真央', 'ロール', '持ち帰り', '笑顔', '番組', '用意', 'フリップ', 'ママ', '浅田', '感謝', '一人', 'ママ', 'ありがとう', '演技', '母', 'お疲れ様', '金メダル', 'じゃなくて', '嬉しい', '言葉', '浅田', '最愛', '母', '金メダル', 'プレゼント', '思い', '強い', 'インタビュアー', 'お母さん', '思い', 'はい', 'さっき', '笑顔', '一変', '静か', '涙']\n",
      "870\n",
      "------------------dokujo-tsushin-4799908.txt------------------\n",
      "['結婚', '結婚', '予備校', '婚活', '学校', 'ネット上', 'キャッチフレーズ', 'クリック', '今年2月', '東京', '青山', '開校', '青山', '結婚', '予備校', 'インフィニスクール', '結婚', '予備校', '婚活', 'ブーム', '一体', '結婚', '予備校', '場所', 'なのか', '結婚', '予備校', '独女', '興味', '結婚', '予備校', '足', 'スクール', '人達', '20代', '30代後半', '会社員', 'メイン', '20代', '一番', '結婚適齢期', '正直', '学校', '貰い手', '…', '時代', '若い', '安定', '志向', '強い', '早い', '結婚', '逆', '30代女性', '可哀想', '世代', '男性', '女性', '人数', '多い', '多い', '学校', '主催', '校長', '佐竹', '悦子', '生徒', '傾向', '前向き', '自分磨き', '好き', '多い', '一般', '結婚相談所', 'ない', '独女通信', '普通', '女性', '多い', '広報', '佐藤留美', '受講', '内容', '通称', '婚活', '女子力', '強化', 'メソッド', '講座', '内容', '紹介', '結婚', '女子力', 'MAKE-UP', '講座', '恋愛', 'メール', 'マナー', '講座', 'デート', '食事', 'マナー', '講座', '女子力', 'MAX', 'コーディネイト', '講座', '特徴', '外見', '磨き', '一般', 'マナー', '向上', '2つ', '結婚', '重要', '外見', '重要', '印象', '最初', '6秒', '%', '有名', '法則', '通り', 'タイプ', '判断', '男性', '女性', 'OK', 'ポケット', 'ない', '逆', '女性', '印象', '多角的', '男性', '評価', '男性', 'ダメ', '第一印象', '悪い', '佐竹', '厳しい', '…', '外見', '良い', 'ダメ', '表層', '大事', 'ない', 'フォロー', '広報', '佐藤', '大事', '訴求', 'モテ', '模試', '恋愛', 'うまい', '結婚', '訴求', '対象', 'つまり', '嫁さん', '佐藤', '嫁さん', '出端', '外見', '磨き', '知名', '嫁さん', '方向', '一生', '一緒に', '生活', 'イメージ', '一言', '快適', '結婚', '刺激', '必要', '男性', '一番', '女性', 'ヒステリー', '佐竹', '生徒', '長い', '交際', '結婚', '彼氏', '気分', '一新', '入校', '女性', '交際', '最低', '2年', '以内', '結婚', '決意', '無理', '気持ち', '1年', '数カ月', '例外', '女性', '早い', '結婚', '自身', '人生設計', '佐竹', '結婚', '講座', '結婚相談所', '機能', '日本', '結婚相談所', '連盟', '所属', '会員', '結婚相手', 'お見合い', '申し込み', '結婚', '戦略', '出会い', '両方', '提供', '仕組み', '費用', 'スクール', 'コース', '3か月', 'カリキュラム', '1年間', '有効', '入会', '20万円', '結婚相手', '紹介', '機能', 'コース', '20万円', '2つ', 'コース', '30万円', '高い', '安い', '判断', '結婚', '独女', '結婚', 'ない', '佐竹', '結婚', 'プロ', '後押し', '結婚', '間違い', '高山', '惠', '青山', '結婚', '予備校', 'インフィニスクール']\n",
      "model saved\n",
      "model saved\n",
      "model saved\n",
      "model saved\n",
      "--------FINISH!!!--------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import mojimoji\n",
    "import codecs\n",
    "\n",
    "class CourpasReader(object):\n",
    "    ROOT_PATH = './data/text/'\n",
    "    DIR_NAMES = ['dokujo-tsushin',\n",
    "                 'kaden-channel',\n",
    "                 'movie-enter']\n",
    "    # 'livedoor-homme'\n",
    "    # 'smax'\n",
    "    DIR_NAMES = ['movie-enter',\n",
    "                 'it-life-hack',\n",
    "                 'kaden-channel',\n",
    "                 'topic-news',\n",
    "                 'peachy',\n",
    "                 'sports-watch',\n",
    "                 'dokujo-tsushin',\n",
    "                 ]\n",
    "    \n",
    "    REMOVE_FILENAMES = 'LICENSE.txt'\n",
    "    \n",
    "    OUT_DIR = './data/ldcourpas'\n",
    "    NAME_TRAIN_DOC = 'train_doclist.list'\n",
    "    NAME_TRAIN_TITLE = 'train_titlelist.list'\n",
    "    NAME_TEST_DOC = 'test_doclist.list'\n",
    "    NAME_TEST_TITLE = 'test_titlelist.list'\n",
    "    \n",
    "    TRAIN_SIZE_PER_GENRE = 500\n",
    "    TEST_SIZE_PER_GENRE = 200\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.extractor = Extractor()\n",
    "    \n",
    "    def __digit_zen_to_han(self, text):\n",
    "        \"\"\"\n",
    "        テキスト内の全角数字を半角数字に変換\n",
    "        \n",
    "        @param text str\n",
    "        @return text_transformed str text内の全角数字を半角数字に変換したもの\n",
    "        \"\"\"\n",
    "        text_transfomed = text\n",
    "        match = re.findall(r'[０-９]+', text_transfomed)\n",
    "        for zen in match:\n",
    "            han = mojimoji.zen_to_han(zen)\n",
    "            text_transfomed = text_transfomed.replace(zen, han, 1)\n",
    "        return text_transfomed\n",
    "    \n",
    "    def __create_doc(self, path):\n",
    "        \"\"\"\n",
    "        @param path str 読み込む記事のパス\n",
    "        \n",
    "        @return doc list 記事を単語単位に分割したもの。\n",
    "        \"\"\"\n",
    "        \n",
    "        doc = []\n",
    "        \n",
    "        with codecs.open(path, 'r', 'utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                # 最初の2行はurlと時刻なのでカット\n",
    "                if i > 1:\n",
    "                    \"\"\"\n",
    "                    # 改行文字を「。」に置き換え\n",
    "                    line = re.sub(r'[\\n]', '。', line)\n",
    "                    # 「。」が2個以上連続してたら１つにする\n",
    "                    line = re.sub(r'[。]+', '。', line)\n",
    "                    # 全角特殊文字\n",
    "                    line = re.sub(r'[！？”“％＆＄＃（）［］]', '', line)\n",
    "                    \"\"\"\n",
    "                    line = line.encode('utf-8').decode('utf-8')\n",
    "                    line = line.strip()\n",
    "                    line = self.__digit_zen_to_han(line)\n",
    "                    line = re.sub(r'[\\s!\"#$%&()=^~{}\\[\\]+*;:@]', '', line)\n",
    "                    if line != '':\n",
    "                        words = self.extractor.extract_words(line)\n",
    "                        doc.extend(words)\n",
    "        return doc\n",
    "    \n",
    "    def __print_doc(self, path):\n",
    "        doc = ''\n",
    "        \n",
    "        with open(path, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                print(line)\n",
    "                \n",
    "                \n",
    "    def __save_obj(self, path, obj):\n",
    "        with open(path, mode='wb') as f:\n",
    "            pickle.dump(obj, f)\n",
    "        print('model saved')\n",
    "    \n",
    "    def extract_doclist(self):\n",
    "        \"\"\"\n",
    "        各ディレクトリから、文書名と文書を抜き出し学習データとテストデータを作成\n",
    "        \"\"\"\n",
    "        train_doclist = []\n",
    "        train_titlelist = []\n",
    "        test_doclist = []\n",
    "        test_titlelist = []\n",
    "        \n",
    "        for dir_name in self.DIR_NAMES:\n",
    "            dir_path = self.ROOT_PATH + dir_name\n",
    "            titles = os.listdir(dir_path)\n",
    "            titles.remove(self.REMOVE_FILENAMES)\n",
    "            print(len(titles))\n",
    "            end = self.TRAIN_SIZE_PER_GENRE + self.TEST_SIZE_PER_GENRE\n",
    "            for i, title in enumerate(sorted(titles[0:end])):\n",
    "                doc = self.__create_doc(dir_path + '/' + title)\n",
    "                \n",
    "                if i == 10:\n",
    "                    print('------------------{}------------------'\\\n",
    "                          .format(title))\n",
    "                    print(doc)\n",
    "                    \n",
    "                \n",
    "                if i < self.TRAIN_SIZE_PER_GENRE:\n",
    "                    train_doclist.append(doc)\n",
    "                    train_titlelist.append(title)\n",
    "                else:\n",
    "                    test_doclist.append(doc)\n",
    "                    test_titlelist.append(title)\n",
    "        \n",
    "        self.__save_obj(path=self.OUT_DIR + self.NAME_TRAIN_DOC,\n",
    "                        obj=train_doclist)\n",
    "        \n",
    "        self.__save_obj(path=self.OUT_DIR + self.NAME_TRAIN_TITLE,\n",
    "                        obj=train_titlelist)\n",
    "        \n",
    "        self.__save_obj(path=self.OUT_DIR + self.NAME_TEST_DOC,\n",
    "                        obj=test_doclist)\n",
    "        \n",
    "        self.__save_obj(path=self.OUT_DIR + self.NAME_TEST_TITLE,\n",
    "                        obj=test_titlelist)\n",
    "        print('--------FINISH!!!--------')\n",
    "\n",
    "        \n",
    "def main():\n",
    "    reader = CourpasReader()\n",
    "    reader.extract_doclist()\n",
    "    \n",
    "\n",
    "main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 映画レビューのデータセット\n",
    "\n",
    "IMDbWebサイトの映画レビューデータセット([ここ](http://ai.stanford.edu/~amaas/data/sentiment/)から入手できる)を利用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "import pickle\n",
    "\n",
    "reviews_train = load_files('data/aclImdb/train/')\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "text_train = [doc.replace(b'<br />', b'') for doc in text_train]\n",
    "\n",
    "reviews_test = load_files('data/aclImdb/test/')\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "text_test = [doc.replace(b'<br />', b'') for doc in text_test]\n",
    "\n",
    "imdb_dir = './data/imdb/'\n",
    "with open(imdb_dir + 'text_test.list', mode='wb') as f:\n",
    "    pickle.dump(text_test, f)\n",
    "with open(imdb_dir + 'y_test.list', mode='wb') as f:\n",
    "    pickle.dump(y_test, f)\n",
    "with open(imdb_dir + 'text_train', mode='wb') as f:\n",
    "    pickle.dump(text_train, f)\n",
    "with open(imdb_dir + 'y_train', mode='wb') as f:\n",
    "    pickle.dump(y_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.9705810546875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "imdb_dir = './data/imdb/'\n",
    "with open(imdb_dir + 'text_test.list', mode='rb') as f:\n",
    "    text_test = pickle.load(f)\n",
    "with open(imdb_dir + 'y_test.list', mode='rb') as f:\n",
    "    y_test = pickle.load(f)\n",
    "with open(imdb_dir + 'text_train', mode='rb') as f:\n",
    "    text_train = pickle.load(f)\n",
    "with open(imdb_dir + 'y_train', mode='rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "    \n",
    "\n",
    "vect = CountVectorizer(max_features=10000, max_df=.15)\n",
    "X = vect.fit_transform(text_train[0:2700])\n",
    "\n",
    "\n",
    "lda_sklearn = LatentDirichletAllocation(n_components=10, learning_method='batch',\n",
    "                                max_iter=25, random_state=0)\n",
    "start = time.time()\n",
    "document_topics = lda_sklearn.fit_transform(X)\n",
    "elapsed_time = time.time() - start\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nmglearn.tools.print_topics(topics=range(10), feature_names=feature_names,\\n                          sorting=sorting, topics_per_chunk=5, n_words=10)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorting = np.argsort(lda_sklearn.components_, axis=1)[:, ::-1]\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "print(lda_sklearn.components_.shape)\n",
    "\"\"\"\n",
    "mglearn.tools.print_topics(topics=range(10), feature_names=feature_names,\n",
    "                          sorting=sorting, topics_per_chunk=5, n_words=10)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished making vocab\n",
      "start iterating\n",
      "1 iter finished !\n",
      "2 iter finished !\n",
      "3 iter finished !\n",
      "4 iter finished !\n",
      "5 iter finished !\n",
      "6 iter finished !\n",
      "7 iter finished !\n",
      "8 iter finished !\n",
      "9 iter finished !\n",
      "10 iter finished !\n",
      "11 iter finished !\n",
      "12 iter finished !\n",
      "13 iter finished !\n",
      "14 iter finished !\n",
      "15 iter finished !\n",
      "16 iter finished !\n",
      "17 iter finished !\n",
      "18 iter finished !\n",
      "19 iter finished !\n",
      "20 iter finished !\n",
      "21 iter finished !\n",
      "22 iter finished !\n",
      "23 iter finished !\n",
      "24 iter finished !\n",
      "25 iter finished !\n",
      "26 iter finished !\n",
      "27 iter finished !\n",
      "28 iter finished !\n",
      "29 iter finished !\n",
      "30 iter finished !\n",
      "31 iter finished !\n",
      "32 iter finished !\n",
      "33 iter finished !\n",
      "34 iter finished !\n",
      "35 iter finished !\n",
      "36 iter finished !\n",
      "37 iter finished !\n",
      "38 iter finished !\n",
      "39 iter finished !\n",
      "40 iter finished !\n",
      "41 iter finished !\n",
      "42 iter finished !\n",
      "43 iter finished !\n",
      "44 iter finished !\n",
      "45 iter finished !\n",
      "46 iter finished !\n",
      "47 iter finished !\n",
      "48 iter finished !\n",
      "49 iter finished !\n",
      "50 iter finished !\n",
      "51 iter finished !\n",
      "52 iter finished !\n",
      "53 iter finished !\n",
      "54 iter finished !\n",
      "55 iter finished !\n",
      "56 iter finished !\n",
      "57 iter finished !\n",
      "58 iter finished !\n",
      "59 iter finished !\n",
      "60 iter finished !\n",
      "61 iter finished !\n",
      "62 iter finished !\n",
      "63 iter finished !\n",
      "64 iter finished !\n",
      "65 iter finished !\n",
      "66 iter finished !\n",
      "67 iter finished !\n",
      "68 iter finished !\n",
      "69 iter finished !\n",
      "70 iter finished !\n",
      "71 iter finished !\n",
      "72 iter finished !\n",
      "73 iter finished !\n",
      "74 iter finished !\n",
      "75 iter finished !\n",
      "76 iter finished !\n",
      "77 iter finished !\n",
      "78 iter finished !\n",
      "79 iter finished !\n",
      "80 iter finished !\n",
      "81 iter finished !\n",
      "82 iter finished !\n",
      "83 iter finished !\n",
      "84 iter finished !\n",
      "85 iter finished !\n",
      "86 iter finished !\n",
      "87 iter finished !\n",
      "88 iter finished !\n",
      "89 iter finished !\n",
      "90 iter finished !\n",
      "91 iter finished !\n",
      "92 iter finished !\n",
      "93 iter finished !\n",
      "94 iter finished !\n",
      "95 iter finished !\n",
      "96 iter finished !\n",
      "97 iter finished !\n",
      "98 iter finished !\n",
      "99 iter finished !\n",
      "100 iter finished !\n",
      "101 iter finished !\n",
      "102 iter finished !\n",
      "103 iter finished !\n",
      "104 iter finished !\n",
      "105 iter finished !\n",
      "106 iter finished !\n",
      "107 iter finished !\n",
      "108 iter finished !\n",
      "109 iter finished !\n",
      "110 iter finished !\n",
      "111 iter finished !\n",
      "112 iter finished !\n",
      "113 iter finished !\n",
      "114 iter finished !\n",
      "115 iter finished !\n",
      "116 iter finished !\n",
      "117 iter finished !\n",
      "118 iter finished !\n",
      "119 iter finished !\n",
      "120 iter finished !\n",
      "121 iter finished !\n",
      "122 iter finished !\n",
      "123 iter finished !\n",
      "124 iter finished !\n",
      "125 iter finished !\n",
      "126 iter finished !\n",
      "127 iter finished !\n",
      "128 iter finished !\n",
      "129 iter finished !\n",
      "130 iter finished !\n",
      "131 iter finished !\n",
      "132 iter finished !\n",
      "133 iter finished !\n",
      "134 iter finished !\n",
      "135 iter finished !\n",
      "136 iter finished !\n",
      "137 iter finished !\n",
      "138 iter finished !\n",
      "139 iter finished !\n",
      "140 iter finished !\n",
      "141 iter finished !\n",
      "142 iter finished !\n",
      "143 iter finished !\n",
      "144 iter finished !\n",
      "145 iter finished !\n",
      "146 iter finished !\n",
      "147 iter finished !\n",
      "148 iter finished !\n",
      "149 iter finished !\n",
      "150 iter finished !\n",
      "151 iter finished !\n",
      "152 iter finished !\n",
      "153 iter finished !\n",
      "154 iter finished !\n",
      "155 iter finished !\n",
      "156 iter finished !\n",
      "157 iter finished !\n",
      "158 iter finished !\n",
      "159 iter finished !\n",
      "160 iter finished !\n",
      "161 iter finished !\n",
      "162 iter finished !\n",
      "163 iter finished !\n",
      "164 iter finished !\n",
      "165 iter finished !\n",
      "166 iter finished !\n",
      "167 iter finished !\n",
      "168 iter finished !\n",
      "169 iter finished !\n",
      "170 iter finished !\n",
      "171 iter finished !\n",
      "172 iter finished !\n",
      "173 iter finished !\n",
      "174 iter finished !\n",
      "175 iter finished !\n",
      "176 iter finished !\n",
      "177 iter finished !\n",
      "178 iter finished !\n",
      "179 iter finished !\n",
      "180 iter finished !\n",
      "181 iter finished !\n",
      "182 iter finished !\n",
      "183 iter finished !\n",
      "184 iter finished !\n",
      "185 iter finished !\n",
      "186 iter finished !\n",
      "187 iter finished !\n",
      "188 iter finished !\n",
      "189 iter finished !\n",
      "190 iter finished !\n",
      "191 iter finished !\n",
      "192 iter finished !\n",
      "193 iter finished !\n",
      "194 iter finished !\n",
      "195 iter finished !\n",
      "196 iter finished !\n",
      "197 iter finished !\n",
      "198 iter finished !\n",
      "199 iter finished !\n",
      "200 iter finished !\n",
      "201 iter finished !\n",
      "202 iter finished !\n",
      "203 iter finished !\n",
      "204 iter finished !\n",
      "205 iter finished !\n",
      "206 iter finished !\n",
      "207 iter finished !\n",
      "208 iter finished !\n",
      "209 iter finished !\n",
      "210 iter finished !\n",
      "211 iter finished !\n",
      "212 iter finished !\n",
      "213 iter finished !\n",
      "214 iter finished !\n",
      "215 iter finished !\n",
      "216 iter finished !\n",
      "217 iter finished !\n",
      "218 iter finished !\n",
      "219 iter finished !\n",
      "220 iter finished !\n",
      "221 iter finished !\n",
      "222 iter finished !\n",
      "223 iter finished !\n",
      "224 iter finished !\n",
      "225 iter finished !\n",
      "226 iter finished !\n",
      "227 iter finished !\n",
      "228 iter finished !\n",
      "229 iter finished !\n",
      "230 iter finished !\n",
      "231 iter finished !\n",
      "232 iter finished !\n",
      "233 iter finished !\n",
      "234 iter finished !\n",
      "235 iter finished !\n",
      "236 iter finished !\n",
      "237 iter finished !\n",
      "238 iter finished !\n",
      "239 iter finished !\n",
      "240 iter finished !\n",
      "241 iter finished !\n",
      "242 iter finished !\n",
      "243 iter finished !\n",
      "244 iter finished !\n",
      "245 iter finished !\n",
      "246 iter finished !\n",
      "247 iter finished !\n",
      "248 iter finished !\n",
      "249 iter finished !\n",
      "250 iter finished !\n",
      "-----topic 0-----\n",
      "school, pdf:0.010392341297238527\n",
      "anything, pdf:0.009196266500014956\n",
      "down, pdf:0.00876070480385666\n",
      "ll, pdf:0.008338130798102594\n",
      "high, pdf:0.008148711366119731\n",
      "old, pdf:0.007958866724819168\n",
      "else, pdf:0.007640743900193343\n",
      "pretty, pdf:0.0075744528310175314\n",
      "shot, pdf:0.006943927586296163\n",
      "half, pdf:0.00631519963459748\n",
      "-----topic 1-----\n",
      "actors, pdf:0.011770334285600719\n",
      "same, pdf:0.00839922138935371\n",
      "another, pdf:0.00816785465379919\n",
      "come, pdf:0.008034333959057196\n",
      "want, pdf:0.007951990874654548\n",
      "part, pdf:0.0070599490100998\n",
      "performance, pdf:0.007011621456348117\n",
      "book, pdf:0.006838815738510508\n",
      "thought, pdf:0.006737718813556429\n",
      "again, pdf:0.006516815197775836\n",
      "-----topic 2-----\n",
      "family, pdf:0.016829087117781364\n",
      "john, pdf:0.014685305058184678\n",
      "young, pdf:0.010910137393863272\n",
      "old, pdf:0.009049158502311899\n",
      "through, pdf:0.007176235568896504\n",
      "horror, pdf:0.006849427908397877\n",
      "ve, pdf:0.006395701543887506\n",
      "plays, pdf:0.006213432069965486\n",
      "town, pdf:0.005615112427719022\n",
      "however, pdf:0.0053938214972475106\n",
      "-----topic 3-----\n",
      "show, pdf:0.017998155328781785\n",
      "re, pdf:0.008098724311017266\n",
      "script, pdf:0.007791394545857945\n",
      "while, pdf:0.007712672025583208\n",
      "now, pdf:0.007649306176794125\n",
      "instead, pdf:0.0073469239210326096\n",
      "audience, pdf:0.0072472876119263445\n",
      "take, pdf:0.006469195964412634\n",
      "going, pdf:0.0062183639018924056\n",
      "quite, pdf:0.0057871795851248495\n",
      "-----topic 4-----\n",
      "years, pdf:0.01025780103482702\n",
      "real, pdf:0.009656508312926582\n",
      "us, pdf:0.009476897930371755\n",
      "father, pdf:0.009057803473114976\n",
      "wife, pdf:0.007437286670424905\n",
      "girl, pdf:0.006936290962587126\n",
      "our, pdf:0.006824400028570107\n",
      "between, pdf:0.006621144874045786\n",
      "while, pdf:0.006595792407502268\n",
      "dead, pdf:0.006116481783608717\n",
      "-----topic 5-----\n",
      "work, pdf:0.011599237981361471\n",
      "look, pdf:0.007891510461306293\n",
      "real, pdf:0.006901252124159368\n",
      "men, pdf:0.006888210217343178\n",
      "interesting, pdf:0.006669444233988167\n",
      "perhaps, pdf:0.00662195759410737\n",
      "far, pdf:0.006500994886295128\n",
      "fun, pdf:0.006422768544429241\n",
      "overall, pdf:0.006139365214606771\n",
      "true, pdf:0.0061180843620305975\n",
      "-----topic 6-----\n",
      "world, pdf:0.010488199943941881\n",
      "between, pdf:0.009577570164194004\n",
      "new, pdf:0.009334921384999617\n",
      "role, pdf:0.00833504450522494\n",
      "yet, pdf:0.007147839929198739\n",
      "american, pdf:0.0063649545524784275\n",
      "michael, pdf:0.006293629236310368\n",
      "plays, pdf:0.0060554390640057375\n",
      "us, pdf:0.005752422028747618\n",
      "might, pdf:0.005495235181594568\n",
      "-----topic 7-----\n",
      "fact, pdf:0.011362806279335348\n",
      "however, pdf:0.009733832756305202\n",
      "original, pdf:0.009571626286789875\n",
      "own, pdf:0.009237488895748169\n",
      "both, pdf:0.008410425414353783\n",
      "cast, pdf:0.00821419961434268\n",
      "nothing, pdf:0.007099640998243064\n",
      "full, pdf:0.00628774212848447\n",
      "along, pdf:0.005925826178712391\n",
      "special, pdf:0.005912955798592057\n",
      "-----topic 8-----\n",
      "funny, pdf:0.01303919714706262\n",
      "series, pdf:0.011855057382210817\n",
      "action, pdf:0.01090572784803605\n",
      "re, pdf:0.008769713020441973\n",
      "episode, pdf:0.008364007641434176\n",
      "now, pdf:0.008155336471754234\n",
      "goes, pdf:0.00782579049890044\n",
      "gets, pdf:0.0074656789281446236\n",
      "quite, pdf:0.007319459552435053\n",
      "fun, pdf:0.006910282332648786\n",
      "-----topic 9-----\n",
      "ve, pdf:0.013031836184732314\n",
      "lot, pdf:0.012007513406613761\n",
      "got, pdf:0.00890028266577597\n",
      "music, pdf:0.008266177674842785\n",
      "didn, pdf:0.007983662189300774\n",
      "10, pdf:0.0072728338990441675\n",
      "guy, pdf:0.007260101131300677\n",
      "worst, pdf:0.006962748581704344\n",
      "saw, pdf:0.006944820876904609\n",
      "through, pdf:0.006782134176872196\n"
     ]
    }
   ],
   "source": [
    "lda = LDA(n_iter=250, n_topic=10, max_df=.15, lang='en')\n",
    "lda.fit(text_train[0:2000])\n",
    "lda.print_topn_pertopic(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_test():\n",
    "    \"\"\"\n",
    "    @return train list 学習用の文書集合\n",
    "    @return test list テスト用の文書集合\n",
    "    \"\"\"\n",
    "    read_dir = './data/ldcourpas/'\n",
    "    train_doc_name = 'train_doclist.list'\n",
    "    test_doc_name = 'test_doclist.list'\n",
    "    \n",
    "    with open(read_dir + train_doc_name, mode='rb') as f:\n",
    "        train = pickle.load(f)\n",
    "    with open(read_dir + test_doc_name, mode='rb') as f:\n",
    "        test = pickle.load(f)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pickle\n",
    "\n",
    "train, test = load_train_test()\n",
    "analyzer = lambda words: words\n",
    "vect = CountVectorizer(max_features=10000, max_df=.15, analyzer=analyzer)\n",
    "X = vect.fit_transform(train)\n",
    "lda_sklearn = LatentDirichletAllocation(n_components=10, learning_method='batch',\n",
    "                                max_iter=25, random_state=0)\n",
    "document_topics = lda_sklearn.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v:  1170\n",
      "d:  0\n",
      "n_v:  1\n",
      "X[d, v]:  1\n",
      "v:  7074\n",
      "d:  501\n",
      "n_v:  1\n",
      "X[d, v]:  1\n",
      "v:  213\n",
      "d:  1217\n",
      "n_v:  8\n",
      "X[d, v]:  8\n",
      "v:  4298\n",
      "d:  1693\n",
      "n_v:  1\n",
      "X[d, v]:  1\n",
      "v:  7172\n",
      "d:  2337\n",
      "n_v:  1\n",
      "X[d, v]:  1\n",
      "scipy sparse: 0.10068202018737793 [sec]\n",
      "266639\n",
      "scipy sparse: 0.7507660388946533 [sec]\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix, find\n",
    "import time\n",
    "\n",
    "\"\"\"\n",
    "count = 0\n",
    "start = time.time()\n",
    "for d, doc in enumerate(X):\n",
    "    _, col, value = find(doc)\n",
    "    for v, n_v in zip(col, value):\n",
    "        print(doc[d, v])\n",
    "elapsed_time = time.time() - start\n",
    "print('scipy sparse: {} [sec]'.format(elapsed_time))\n",
    "print(count)\n",
    "\"\"\"\n",
    "\n",
    "count = 0\n",
    "start = time.time()\n",
    "n_docs, n_features = X.shape\n",
    "indices = X.indices\n",
    "indptr = X.indptr\n",
    "data = X.data\n",
    "for d in range(n_docs):\n",
    "    vs = indices[indptr[d]:indptr[d+1]]\n",
    "    n_vs = data[indptr[d]:indptr[d+1]]\n",
    "    for v, n_v in zip(vs, n_vs):\n",
    "        if count % 55555 == 0:\n",
    "            print('v: ', v)\n",
    "            print('d: ', d)\n",
    "            print('n_v: ', n_v)\n",
    "            print('X[d, v]: ', X[d,v])\n",
    "        count += 1\n",
    "elapsed_time = time.time() - start\n",
    "print('scipy sparse: {} [sec]'.format(elapsed_time))\n",
    "print(count)\n",
    "\n",
    "\n",
    "count = 0\n",
    "start = time.time()\n",
    "n_docs, n_features = X.shape\n",
    "indices = X.indices\n",
    "indptr = X.indptr\n",
    "data = X.data\n",
    "for d in range(n_docs):\n",
    "    vs = indices[indptr[d]:indptr[d+1]]\n",
    "    start_idx = indptr[d]\n",
    "    for i, v in enumerate(vs):\n",
    "        n_v = data[start_idx + i]\n",
    "        count += 1\n",
    "elapsed_time = time.time() - start\n",
    "print('scipy sparse: {} [sec]'.format(elapsed_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
