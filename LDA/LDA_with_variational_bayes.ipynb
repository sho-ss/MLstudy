{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA with Variational Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train & test データロード\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_train_test():\n",
    "    \"\"\"\n",
    "    @return train list 学習用の文書集合\n",
    "    @return test list テスト用の文書集合\n",
    "    \"\"\"\n",
    "    read_dir = './data/ldcourpas/'\n",
    "    train_doc_name = 'train_doclist.list'\n",
    "    test_doc_name = 'test_doclist.list'\n",
    "    \n",
    "    with open(read_dir + train_doc_name, mode='rb') as f:\n",
    "        train = pickle.load(f)\n",
    "    with open(read_dir + test_doc_name, mode='rb') as f:\n",
    "        test = pickle.load(f)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pickle\n",
    "\n",
    "train, test = load_train_test()\n",
    "analyzer = lambda words: words\n",
    "vect = CountVectorizer(max_features=10000, min_df=.01, max_df=.40, analyzer=analyzer)\n",
    "X = vect.fit_transform(train)\n",
    "id2word = {v: k for k, v in vect.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.special as special\n",
    "\n",
    "\n",
    "def _dirichlet_expectation_1d(arr):\n",
    "    \"\"\"\n",
    "    calcurate (E[log(vars)]), vars = Dir(vars|arr)\n",
    "    \"\"\"\n",
    "    sum_arr = arr.sum()\n",
    "    return special.psi(arr) - special.psi(sum_arr)\n",
    "\n",
    "def _dirichlet_expectation_2d(arr):\n",
    "    \"\"\"\n",
    "    calcurate E[log(vars)], vars ~ Dir(vars|arr)\n",
    "    \"\"\"\n",
    "    sum_axis1 = arr.sum(axis=1).reshape(-1, 1)\n",
    "    return special.psi(arr) - special.psi(sum_axis1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:LDA:train start!\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "import scipy.special as special\n",
    "import scipy.stats as stats\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "\n",
    "EPS = np.finfo(np.float).eps\n",
    "\n",
    "\n",
    "def _get_n_jobs(n_jobs):\n",
    "    if n_jobs < 0:\n",
    "        return max(cpu_count() - 1, 1)\n",
    "    elif n_jobs == 0:\n",
    "        ValueError('n_jobs == 0 doesn\\'t meaning')\n",
    "    else:\n",
    "        return n_jobs\n",
    "    \n",
    "\n",
    "def gen_slices(length, n):\n",
    "    \"\"\"\n",
    "    divide idx[0:length] into n\n",
    "    \"\"\"\n",
    "    idx = np.arange(length)\n",
    "    nums = [(length + i) // n for i in range(n)]\n",
    "    \n",
    "    start = 0\n",
    "    for num in nums:\n",
    "        end = start + num\n",
    "        yield idx[start: end]\n",
    "        start = end\n",
    "\n",
    "\n",
    "def mean_change(arr1, arr2):\n",
    "    \"\"\"\n",
    "    calculate mean abs difference between two arrays.\n",
    "    \"\"\"\n",
    "    size = arr1.shape[0]\n",
    "    return np.abs(arr1 - arr2).sum() / size\n",
    "\n",
    "def _init_doctopic(n_features, n_topics, alpha, nd):\n",
    "    phi_d = np.ones((n_features, n_topics)) / n_topics\n",
    "    gamma_d = alpha + nd / n_topics\n",
    "    return phi_d, gamma_d\n",
    "\n",
    "def _update_doctopic_ditribution(X, max_iter, alpha,\n",
    "                                 exp_topic_word_distr,\n",
    "                                 mean_change_tol,\n",
    "                                 random_state,\n",
    "                                 cal_suff_stats):\n",
    "    n_docs, n_features = X.shape\n",
    "    n_topics = exp_topic_word_distr.shape[0]\n",
    "    \n",
    "    indices = X.indices\n",
    "    indptr = X.indptr\n",
    "    data = X.data\n",
    "    \n",
    "    # In the literature, doc_topic is called 'γdk'\n",
    "    doc_topic_distr =  doc_topic_distr = random_state.gamma(100.,\n",
    "                                                            0.01,\n",
    "                                                            (n_docs, n_topics))\n",
    "    exp_doc_topic_ = np.exp(_dirichlet_expectation_2d(doc_topic_distr))\n",
    "    \n",
    "    suff_stats = np.zeros(exp_topic_word_distr.shape) if cal_suff_stats else None\n",
    "    \n",
    "    for d in range(n_docs):\n",
    "        ids = indices[indptr[d]:indptr[d + 1]]\n",
    "        ndw = data[indptr[d]:indptr[d + 1]]\n",
    "        \n",
    "        doc_topic_d = doc_topic_distr[d]\n",
    "        exp_doc_topic_d = exp_doc_topic_[d]\n",
    "        exp_topic_word_d = exp_topic_word_distr[:, ids]\n",
    "        \n",
    "        for _ in range(max_iter):\n",
    "            last_d = doc_topic_d\n",
    "            \n",
    "            norm_phi = np.dot(exp_doc_topic_d, exp_topic_word_d)\n",
    "            \n",
    "            doc_topic_d = alpha + (exp_doc_topic_d *\\\n",
    "                           np.dot(ndw/norm_phi, exp_topic_word_d.T))\n",
    "            \n",
    "            exp_doc_topic_d = np.exp(_dirichlet_expectation_1d(doc_topic_d))\n",
    "            \n",
    "            # check convergence\n",
    "            if mean_change(doc_topic_d, last_d) < mean_change_tol:\n",
    "                break\n",
    "        \n",
    "        doc_topic_distr[d] = doc_topic_d\n",
    "        \n",
    "        if cal_suff_stats:\n",
    "            norm_phi = np.dot(exp_doc_topic_d, exp_topic_word_d)\n",
    "            suff_stats[:, ids] += ndw/norm_phi\\\n",
    "                                    * exp_topic_word_d\\\n",
    "                                    * exp_doc_topic_d.reshape(-1, 1)\n",
    "    \n",
    "    return doc_topic_distr, suff_stats\n",
    "    \n",
    "\n",
    "class LDA(object):\n",
    "    PRINT_EVERY = 20\n",
    "    def __init__(self, n_topics=10, outer_iter=10, max_update_iter=100,\n",
    "                 mean_change_tol=1e-3, n_jobs=-1, verbose=0,\n",
    "                 logger=None):\n",
    "        self.n_topics = n_topics\n",
    "        self.outer_iter = outer_iter\n",
    "        self.max_update_iter = max_update_iter\n",
    "        self.mean_change_tol = mean_change_tol\n",
    "        self.n_jobs = n_jobs\n",
    "        self.verbose = verbose\n",
    "        self.logger = logger\n",
    "        self.random_state = np.random.mtrand._rand\n",
    "    \n",
    "    def _initialize(self, X):\n",
    "        n_docs, n_features = X.shape\n",
    "        \n",
    "        self.alpha = np.ones(self.n_topics) * 0.1\n",
    "        self.eta = 10 / n_features\n",
    "        \n",
    "        init_gamma = 100.\n",
    "        init_var = 1. / init_gamma\n",
    "        \n",
    "        # In the literature, 'lambda'\n",
    "        self.topic_ = self.random_state.gamma(init_gamma,\n",
    "                                             init_var,\n",
    "                                             (self.n_topics, n_features))\n",
    "        \n",
    "        # In the literature, 'exp(E[log(beta))])\n",
    "        self.exp_topic_word_distr = np.exp(\n",
    "            _dirichlet_expectation_2d(self.topic_))\n",
    "        \n",
    "        indices = X.indices\n",
    "        indptr = X.indptr\n",
    "        data = X.data\n",
    "        self.nd = np.zeros(n_docs)\n",
    "        for d in range(n_docs):\n",
    "            ndw = data[indptr[d]:indptr[d + 1]]\n",
    "            self.nd[d] = ndw.sum()\n",
    "        \n",
    "    def _update_dirichlet_param(self, doc_topic):\n",
    "        \"\"\"\n",
    "        update alpha\n",
    "\n",
    "        @param ave_ndz ndarray ndzのサンプル平均\n",
    "        \"\"\"\n",
    "        e_ndk = doc_topic - self.alpha\n",
    "        n_docs = e_ndk.shape[0]\n",
    "        sum_alpha = self.alpha.sum()\n",
    "        \n",
    "        numes = (special.psi(e_ndk + self.alpha).sum(axis=0)\\\n",
    "                    - n_docs*special.psi(self.alpha))*self.alpha\n",
    "        \n",
    "        denom = special.psi(self.nd + sum_alpha).sum()\\\n",
    "                        - n_docs*special.psi(sum_alpha)\n",
    "        \n",
    "        self.alpha = numes / denom\n",
    "        \n",
    "    def _e_step(self, X, parallel=None):\n",
    "        \"\"\"\n",
    "        e-step in EM.\n",
    "        \"\"\"\n",
    "        n_jobs = _get_n_jobs(self.n_jobs)\n",
    "        if parallel is None:\n",
    "            parallel = Parallel(n_jobs=n_jobs, verbose=max(0,\n",
    "                                                           self.verbose-1))\n",
    "        \n",
    "        cal_suff_stats = True\n",
    "        results = parallel(\n",
    "            delayed(_update_doctopic_ditribution)(X[idx_slice, :],\n",
    "                                                  self.max_update_iter,\n",
    "                                                  self.alpha,\n",
    "                                                  self.exp_topic_word_distr,\n",
    "                                                  self.mean_change_tol,\n",
    "                                                  self.random_state,\n",
    "                                                  cal_suff_stats)\n",
    "            for idx_slice in gen_slices(X.shape[0], n_jobs))\n",
    "        \n",
    "        doc_topics, suff_stats_list = zip(*results)\n",
    "        doc_topic = np.vstack(doc_topics)\n",
    "        \n",
    "        if cal_suff_stats:\n",
    "            suff_stats = np.zeros(self.exp_topic_word_distr.shape)\n",
    "            for stats in suff_stats_list:\n",
    "                suff_stats += stats\n",
    "        else:\n",
    "            suff_stats = None\n",
    "        \n",
    "        return doc_topic, suff_stats\n",
    "        \n",
    "    def _em_step(self, X):\n",
    "        \"\"\"\n",
    "        EM update for 1 iteration.\n",
    "        \n",
    "        \"\"\"\n",
    "        doc_topic, suff_stats = self._e_step(X)\n",
    "        \n",
    "        self.topic_ = self.eta + suff_stats\n",
    "        self.exp_topic_word_distr = np.exp(\n",
    "            _dirichlet_expectation_2d(self.topic_))\n",
    "        \n",
    "        return\n",
    "        # update α\n",
    "        self._update_dirichlet_param(doc_topic)\n",
    "        \n",
    "    def fit(self, X):\n",
    "        n_docs, n_features = X.shape\n",
    "        \n",
    "        self._initialize(X)\n",
    "        \n",
    "        if self.logger:\n",
    "            self.logger.info('train start!')\n",
    "        \n",
    "        for out_s in range(self.outer_iter):\n",
    "            start = time.time()\n",
    "            \n",
    "            self._em_step(X)\n",
    "            \n",
    "            elapsed_time = time.time() - start\n",
    "            if self.logger:\n",
    "                self.logger.info('total {} iter finish!'.format(out_s+1))\n",
    "                self.logger.info('elapsed time: {:.3f} [sec]'\\\n",
    "                                 .format(elapsed_time))\n",
    "    \n",
    "    def print_topn_words(self, n, id2word):\n",
    "        index = np.arange(n) + 1\n",
    "        df = pd.DataFrame(data=[], index=index)\n",
    "        for k in range(self.n_topics):\n",
    "            idx_descend = self.topic_[k].argsort()[::-1]\n",
    "            top_n = [id2word[idx] for idx in idx_descend[:n]]\n",
    "            df['topic{}'.format(k)] = top_n\n",
    "        \n",
    "        display(df)\n",
    "        \n",
    "\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "    logger = logging.getLogger('LDA')\n",
    "    lda = LDA(outer_iter=25, max_update_iter=100, logger=logger)\n",
    "    lda.fit(X)\n",
    "    lda.print_topn_words(n=10, id2word=id2word)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
