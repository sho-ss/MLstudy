{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA with Variational Bayes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train & test データロード\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_train_test():\n",
    "    \"\"\"\n",
    "    @return train list 学習用の文書集合\n",
    "    @return test list テスト用の文書集合\n",
    "    \"\"\"\n",
    "    read_dir = './data/ldcourpas/'\n",
    "    train_doc_name = 'train_doclist.list'\n",
    "    test_doc_name = 'test_doclist.list'\n",
    "    \n",
    "    with open(read_dir + train_doc_name, mode='rb') as f:\n",
    "        train = pickle.load(f)\n",
    "    with open(read_dir + test_doc_name, mode='rb') as f:\n",
    "        test = pickle.load(f)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pickle\n",
    "\n",
    "train, test = load_train_test()\n",
    "analyzer = lambda words: words\n",
    "vect = CountVectorizer(max_features=10000, min_df=.01, max_df=.40, analyzer=analyzer)\n",
    "X = vect.fit_transform(train)\n",
    "id2word = {v: k for k, v in vect.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.special as special\n",
    "\n",
    "\n",
    "def _dirichlet_expectation_1d(arr):\n",
    "    \"\"\"\n",
    "    calcureta E[log(theta)]. theta ~ Dir(theta|arr)\n",
    "    \"\"\"\n",
    "    sum_arr = arr.sum()\n",
    "    return special.psi(arr) - special.psi(sum_arr)\n",
    "\n",
    "def _dirichlet_expectation_2d(arr):\n",
    "    \"\"\"\n",
    "    calcurate E[log(theta)]. theta ~ Dir(theta|arr)\n",
    "    \"\"\"\n",
    "    sum_arr_ax1 = arr.sum(axis=1).reshape(-1, 1)\n",
    "    return special.psi(arr) - special.psi(sum_arr_ax1)\n",
    "    \n",
    "def mean_change(arr1, arr2):\n",
    "    size = arr1.shape[0]\n",
    "    return np.abs(arr1 - arr2).sum()/size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _get_n_jobs(n_jobs):\n",
    "    if n_jobs < 0:\n",
    "        return max(cpu_count() - 1, 1)\n",
    "    elif n_jobs == 0:\n",
    "        ValueError('n_jobs == 0 doesn\\'t meaning')\n",
    "    else:\n",
    "        return n_jobs\n",
    "    \n",
    "\n",
    "def gen_slices(length, n):\n",
    "    \"\"\"\n",
    "    divide idx[0:length] into n\n",
    "    \"\"\"\n",
    "    idx = np.arange(length)\n",
    "    nums = [(length + i) // n for i in range(n)]\n",
    "    \n",
    "    start = 0\n",
    "    for num in nums:\n",
    "        end = start + num\n",
    "        yield idx[start: end]\n",
    "        start = end\n",
    "\n",
    "\n",
    "def _update_doc_topic_distrb(X, max_iter, nd,\n",
    "                             exp_topic_word,\n",
    "                             alpha, beta,\n",
    "                             mean_change_tol):\n",
    "    \n",
    "    n_docs, n_features = X.shape\n",
    "    n_topics = exp_topic_word.shape[0]\n",
    "    \n",
    "    doc_topic_distrb = np.zeros((n_docs, n_topics))\\\n",
    "                        + nd.reshape(-1, 1)\\\n",
    "                        / n_topics\n",
    "    exp_doc_topic = np.exp(_dirichlet_expectation_2d(doc_topic_distrb))\n",
    "    sstats = np.zeros(exp_topic_word.shape)\n",
    "    q = np.zeros((n_docs, n_features, n_topics))\n",
    "    \n",
    "    indices = X.indices\n",
    "    indptr = X.indptr\n",
    "    data = X.data\n",
    "    \n",
    "    for d in range(n_docs):\n",
    "        ids = indices[indptr[d]:indptr[d + 1]]\n",
    "        nds = data[indptr[d]:indptr[d + 1]]\n",
    "        \n",
    "        doc_topic_d = doc_topic_distrb[d]\n",
    "        for _ in range(max_iter):\n",
    "            last_d = doc_topic_d\n",
    "            for idx_i in ids:\n",
    "                qd = exp_topic_word[:, idx_i] * exp_doc_topic[d]\n",
    "                norm_qd = qd.sum()\n",
    "                q[d, idx_i] = qd / norm_qd\n",
    "            doc_topic_d = q[d].sum(axis=0) + alpha\n",
    "            exp_doc_topic[d] = np.exp(_dirichlet_expectation_1d(doc_topic_d))\n",
    "            if mean_change(last_d, doc_topic_d) < mean_change_tol:\n",
    "                break\n",
    "        doc_topic_distrb[d] = doc_topic_d\n",
    "        sstats += q[d].T\n",
    "    \n",
    "    return doc_topic_distrb, sstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:LDA:train start!\n",
      "INFO:LDA:elapsed: 96.909 [sec]\n",
      "INFO:LDA:elapsed: 63.840 [sec]\n",
      "INFO:LDA:elapsed: 48.654 [sec]\n",
      "INFO:LDA:elapsed: 41.167 [sec]\n",
      "INFO:LDA:elapsed: 36.075 [sec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----topic 0-----\n",
      "自分, pdf:485.2424224462455\n",
      "それ, pdf:428.71597595165167\n",
      "さん, pdf:424.46462906114886\n",
      "いい, pdf:407.4955474022321\n",
      "今, pdf:394.6907466214214\n",
      "もの, pdf:391.51075399945404\n",
      "時, pdf:373.542639552101\n",
      "何, pdf:370.8836047398745\n",
      "私, pdf:357.575289847065\n",
      "年, pdf:336.76383644733943\n",
      "-----topic 1-----\n",
      "MAX, pdf:141.36132271548607\n",
      "関連リンク, pdf:140.00544861634972\n",
      "MAXsmaxjponTwitter, pdf:138.3446791453985\n",
      "エスマックス, pdf:138.13716596652216\n",
      "S, pdf:138.05553606637218\n",
      "執筆, pdf:128.98313968333116\n",
      "発表, pdf:126.40365710133787\n",
      "発売, pdf:108.65174656511633\n",
      "搭載, pdf:101.75930245073198\n",
      "Android, pdf:101.48749777367156\n",
      "-----topic 2-----\n",
      "対応, pdf:210.5648022613041\n",
      "機能, pdf:201.62908220198878\n",
      "ため, pdf:196.8616532282202\n",
      "発売, pdf:194.12609095777674\n",
      "登場, pdf:191.9183135844302\n",
      "関連, pdf:189.48883857137642\n",
      "搭載, pdf:177.81930655055547\n",
      "これ, pdf:175.3500503928074\n",
      "iPhone, pdf:167.28298986415294\n",
      "製品, pdf:156.17040239028807\n",
      "-----topic 3-----\n",
      "多い, pdf:194.68104343491618\n",
      "女性, pdf:192.85535757936117\n",
      "もの, pdf:185.13542503086106\n",
      "ため, pdf:180.8963303073456\n",
      "的, pdf:170.68723436836373\n",
      "そう, pdf:167.79779959994198\n",
      "人気, pdf:141.3221017809946\n",
      "円, pdf:139.05174017688972\n",
      "気, pdf:130.87712483833215\n",
      "高い, pdf:127.22865208862937\n",
      "-----topic 4-----\n",
      "以上, pdf:150.59307135594224\n",
      "HTTP, pdf:134.83590134952235\n",
      "紹介, pdf:130.55858970836758\n",
      "アプリ, pdf:120.12531950574322\n",
      "無料, pdf:117.51245358808097\n",
      "表示, pdf:116.1138693598442\n",
      "今回, pdf:109.42335588059993\n",
      "便利, pdf:105.19313105032167\n",
      "コンテンツ, pdf:104.0263240674445\n",
      "関連リンク, pdf:101.96792369568557\n",
      "-----topic 5-----\n",
      "才, pdf:46.60094092821006\n",
      "職, pdf:43.39925822547372\n",
      "兼美, pdf:43.298484888511986\n",
      "円, pdf:34.278679184368784\n",
      "月日, pdf:33.42172303285179\n",
      "クリスマス, pdf:32.15554009944089\n",
      "期間, pdf:29.307479088206353\n",
      "金, pdf:26.194386962616115\n",
      "使用, pdf:25.508232626466278\n",
      "特集, pdf:25.17448235412424\n",
      "-----topic 6-----\n",
      "関連, pdf:399.22512632881677\n",
      "月日, pdf:353.33248494800335\n",
      "話題, pdf:296.1797825761581\n",
      "公開, pdf:275.9006545442841\n",
      "映画, pdf:272.9790563049575\n",
      "年, pdf:247.47246222971057\n",
      "声, pdf:213.68667275091173\n",
      "日本, pdf:208.23813272993348\n",
      "登場, pdf:189.2778398428721\n",
      "情報, pdf:185.80787485182162\n",
      "-----topic 7-----\n",
      "利用, pdf:103.71240980664638\n",
      "場合, pdf:96.9536992512968\n",
      "執筆, pdf:94.69662844900235\n",
      "S, pdf:94.48104058400897\n",
      "関連リンク, pdf:92.8268846473224\n",
      "MAX, pdf:91.62403716018561\n",
      "エスマックス, pdf:90.9905517236188\n",
      "MAXsmaxjponTwitter, pdf:90.63090661285426\n",
      "端末, pdf:85.54533359265444\n",
      "smartphone, pdf:84.44055063411518\n",
      "-----topic 8-----\n",
      "関連, pdf:48.515284224218114\n",
      "氏, pdf:46.931864109478525\n",
      "話題, pdf:38.654342916126474\n",
      "HTTP, pdf:35.836773000335995\n",
      "回, pdf:34.97893593195877\n",
      "販売元, pdf:31.22134189540104\n",
      "日本, pdf:31.21611108741407\n",
      "代表, pdf:30.71818844032622\n",
      "イベント, pdf:29.866492953391536\n",
      "同氏, pdf:29.678747669953182\n",
      "-----topic 9-----\n",
      "関連, pdf:129.6478841138601\n",
      "情報, pdf:122.6971035367644\n",
      "これ, pdf:116.74294611665582\n",
      "話題, pdf:109.2103857642362\n",
      "声, pdf:96.8351255419558\n",
      "Twitter, pdf:93.19927416781591\n",
      "ネット掲示板, pdf:80.08193705157146\n",
      "多い, pdf:77.69846631761389\n",
      "批判, pdf:77.50990758790327\n",
      "いい, pdf:70.74793083297897\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "import scipy.special as special\n",
    "import scipy.stats as stats\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "\n",
    "\n",
    "class LDA(object):\n",
    "    def __init__(self, max_iter=10, max_update_iter=100,\n",
    "                 n_topics=10, print_every=20, n_jobs=-1,\n",
    "                 verbose=1, mean_change_tol=1e-4, logger=None):\n",
    "        self.max_iter = max_iter\n",
    "        self.max_update_iter = max_update_iter\n",
    "        self.n_topics = n_topics\n",
    "        self.print_every = print_every\n",
    "        self.n_jobs = n_jobs\n",
    "        self.verbose = verbose\n",
    "        self.mean_change_tol = mean_change_tol\n",
    "        self.logger = logger\n",
    "        self.random_state = np.random.mtrand._rand\n",
    "    \n",
    "    def _initialize(self, X):\n",
    "        n_docs, n_features = X.shape\n",
    "        \n",
    "        self.alpha = np.ones(self.n_topics) * 0.1\n",
    "        self.beta = 100 / n_docs\n",
    "        \n",
    "        init_gamma = 100.\n",
    "        init_var = 1. / init_gamma\n",
    "        self.topic_word_ = self.random_state.gamma(init_gamma,\n",
    "                                                        init_var,\n",
    "                                                        (self.n_topics,\n",
    "                                                         n_features))\n",
    "        self.exp_topic_word = np.exp(\n",
    "            _dirichlet_expectation_2d(self.topic_word_))\n",
    "        \n",
    "        self.nd = np.zeros(n_docs)\n",
    "        indices = X.indices\n",
    "        indptr = X.indptr\n",
    "        data = X.data\n",
    "        for d in range(n_docs):\n",
    "            nds = data[indptr[d]:indptr[d + 1]]\n",
    "            self.nd[d] = nds.sum()\n",
    "    \n",
    "    def _update_dirichlet_param(self, doc_topic):\n",
    "        \"\"\"\n",
    "        update alpha\n",
    "\n",
    "        @param ave_ndz ndarray ndzのサンプル平均\n",
    "        \"\"\"\n",
    "        e_ndk = doc_topic - self.alpha\n",
    "        n_docs = e_ndk.shape[0]\n",
    "        sum_alpha = self.alpha.sum()\n",
    "        \n",
    "        numes = (special.psi(e_ndk + self.alpha).sum(axis=0)\\\n",
    "                    - n_docs*special.psi(self.alpha))*self.alpha\n",
    "        \n",
    "        denom = special.psi(self.nd + sum_alpha).sum()\\\n",
    "                        - n_docs*special.psi(sum_alpha)\n",
    "        \n",
    "        self.alpha = numes / denom\n",
    "    \n",
    "    def _e_step(self, X, parallel=None):\n",
    "        \n",
    "        n_jobs = _get_n_jobs(self.n_jobs)\n",
    "        if parallel is None:\n",
    "            parallel = Parallel(n_jobs, verbose=max(0, self.verbose - 1))\n",
    "        \n",
    "        results = parallel(\n",
    "            delayed(_update_doc_topic_distrb)(X[idx_slice, :],\n",
    "                                              self.max_update_iter,\n",
    "                                              self.nd[idx_slice],\n",
    "                                              self.exp_topic_word,\n",
    "                                              self.alpha,\n",
    "                                              self.beta,\n",
    "                                              self.mean_change_tol)\n",
    "            for idx_slice in gen_slices(X.shape[0], n_jobs))\n",
    "                                             \n",
    "        doc_topics, sstats_list = zip(*results)\n",
    "        doc_topic = np.vstack(doc_topics)\n",
    "        \n",
    "        sstats = np.zeros(self.exp_topic_word.shape)\n",
    "        for suff_stats in sstats_list:\n",
    "            sstats += suff_stats\n",
    "        \n",
    "        return doc_topic, sstats\n",
    "    \n",
    "    def _em_step(self, X):\n",
    "        \n",
    "        doc_topic, sstats = self._e_step(X)\n",
    "        self.topic_word_ = self.beta + sstats\n",
    "        self.exp_topic_word = np.exp(\n",
    "            _dirichlet_expectation_2d(self.topic_word_))\n",
    "        self._update_dirichlet_param(doc_topic)\n",
    "    \n",
    "    def fit(self, X):\n",
    "        n_docs, n_features = X.shape\n",
    "        self._initialize(X)\n",
    "        \n",
    "        if self.logger:\n",
    "            self.logger.info('train start!')\n",
    "        for s in range(self.max_iter):\n",
    "            start = time.time()\n",
    "            self._em_step(X)\n",
    "            elapsed_time = time.time() - start\n",
    "            \n",
    "            if self.logger:\n",
    "                self.logger.info('elapsed: {:.3f} [sec]'.format(elapsed_time))\n",
    "            if (s+1) % self.print_every == 0:\n",
    "                self.logger.info('{} iterations finished.'.format(s+1))\n",
    "    \n",
    "    def _sampling_phi(self):\n",
    "        self.phi = np.zeros(self.exp_topic_word.shape)\n",
    "        for k in range(self.n_topics):\n",
    "            self.phi[k, :] = stats.dirichlet.rvs(self.topic_word_[k], size=1)[0]\n",
    "\n",
    "    def print_topn_words(self, n, id2word):\n",
    "        index = np.arange(n) + 1\n",
    "        df = pd.DataFrame(data=[], index=index)\n",
    "        for k in range(self.n_topics):\n",
    "            idx_descend = self.topic_word_[k].argsort()[::-1]\n",
    "            top_n = [id2word[idx] for idx in idx_descend[:n]]\n",
    "            df['topic{}'.format(k)] = top_n\n",
    "        display(df)\n",
    "    \n",
    "    def print_topn_pertopic(self, n=5, vocab=None):\n",
    "        index_to_words = {v: k for k, v in vocab.items()}\n",
    "        for k in range(self.n_topics):\n",
    "            print('-----topic {}-----'.format(k))\n",
    "            index_phi_k = self.topic_word_[k].argsort()[::-1]\n",
    "            for print_num, v in enumerate(index_phi_k):\n",
    "                if print_num >= n:\n",
    "                    break\n",
    "                \n",
    "                print('{}, pdf:{}'.format(index_to_words[v],\n",
    "                                          self.topic_word_[k, v]))\n",
    "\n",
    "\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "    logger = logging.getLogger('LDA')\n",
    "    lda = LDA(max_iter=5, max_update_iter=100, logger=logger)\n",
    "    lda.fit(X)\n",
    "    lda.print_topn_pertopic(n=10, vocab=vect.vocabulary_)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
